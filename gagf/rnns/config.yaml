# Data configuration
data:
  p1: 10 #28
  p2: 10 #28
  k: 4 # GOAL: 5  # sequence length
  batch_size: 1000 #20000 #1000
  seed: 5
  mnist_label: 4 # only if template generated from mnist
  
  mode: sampled # used only for 'offline' training
  num_samples: 100000 # used only for `offline` training

# Model configuration
model:
  hidden_dim: 36 #36
  init_scale: 1.0e-2 # effective init scale related to k (larger k, larger init scale to prevent gradient squashing)
  return_all_outputs: false # true = guided (seq-to-seq), false = non-guided (seq-to-one)
  transform_type: 'quadratic' # 'quadratic' | `multiplicative' 

# Training configuration
training:
  mode: 'online' # 'online' | 'offline'
  epochs: 200 # used only when mode is 'offline'
  num_steps: 500000 #50000 #50000 # 20000 # used only when mode is 'online'

  optimizer: 'adam' # 'adam' | 'hybrid'
  learning_rate: 1.0e-3 #0.0001
  betas: [0.9, 0.999]

  scaling_factor: -3 # for 'hybrid' optimizer only
  
  weight_decay: 0.0
  grad_clip: 0.1
  verbose_interval: 10
  save_param_interval: 100 #1000 #100


device: cuda

# Analysis
analysis:
  checkpoints: [0.0, 0.25, 0.5, 0.75, 1.0] # At 0%, 25%, 50%, 75%, 100% of training
