{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary Group Composition on $C_n$\n",
        "\n",
        "**Group:** Cyclic group $C_n$ of order $p$ (i.e., modular addition mod $p$).  \n",
        "**Task:** Given encodings of two group elements $a, b \\in C_p$, predict the encoding of their product $a + b \\pmod{p}$.  \n",
        "**Sequence length:** $k = 2$ (binary composition).  \n",
        "**Architecture:** `TwoLayerNet` with square nonlinearity.  \n",
        "**Key result:** The network learns one Fourier mode at a time, producing a staircase in the training loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import src.dataset as dataset\n",
        "import src.model as model\n",
        "import src.optimizer as optimizer\n",
        "import src.power as power\n",
        "import src.template as template\n",
        "import src.train as train_mod\n",
        "import src.viz as viz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_MODE = os.environ.get(\"NOTEBOOK_TEST_MODE\", \"0\") == \"1\"\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "p = 11\n",
        "hidden_size = 20 if TEST_MODE else 200\n",
        "epochs = 2 if TEST_MODE else 5000\n",
        "lr = 0.01\n",
        "init_scale = 1e-5\n",
        "\n",
        "FIGURES_DIR = \"figures\"\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Template and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067e5bc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a fixed Cn template with known Fourier structure\n",
        "tpl = template.fixed_cn(\n",
        "    group_size=p,\n",
        "    fourier_coef_mags=[0, 12.5, 10, 7.5, 5, 2.5],\n",
        ")\n",
        "\n",
        "# Mean-center the template\n",
        "tpl = tpl - np.mean(tpl)\n",
        "\n",
        "# Build exhaustive dataset: all p^2 pairs\n",
        "X, Y = dataset.cn_dataset(tpl)\n",
        "\n",
        "# Move to tensors and flatten\n",
        "X_tensor, Y_tensor, device = dataset.move_dataset_to_device_and_flatten(X, Y)\n",
        "\n",
        "ds = TensorDataset(X_tensor, Y_tensor)\n",
        "dataloader = DataLoader(ds, batch_size=len(ds), shuffle=False)\n",
        "\n",
        "print(f\"Group: C_{p}, order {p}\")\n",
        "print(f\"Dataset: {len(ds)} samples (all {p}x{p} pairs)\")\n",
        "print(f\"X shape: {X_tensor.shape}, Y shape: {Y_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26b21e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize template and its power spectrum\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.bar(range(p), tpl, color=\"black\")\n",
        "ax1.set_xlabel(\"Group element\")\n",
        "ax1.set_ylabel(\"Template value\")\n",
        "ax1.set_title(f\"Template $t$ on $C_{{{p}}}$\")\n",
        "\n",
        "pwr, freqs = power.get_power_1d(tpl)\n",
        "ax2.bar(freqs, pwr, color=\"steelblue\")\n",
        "ax2.set_xlabel(\"Frequency\")\n",
        "ax2.set_ylabel(\"Power\")\n",
        "ax2.set_title(\"Power spectrum of template\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/cn_template.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "230b324e",
      "metadata": {},
      "outputs": [],
      "source": [
        "net = model.TwoLayerNet(\n",
        "    group_size=p,\n",
        "    hidden_size=hidden_size,\n",
        "    nonlinearity=\"square\",\n",
        "    init_scale=init_scale,\n",
        ")\n",
        "net = net.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "opt = optimizer.PerNeuronScaledSGD(net, lr=lr, degree=3)\n",
        "\n",
        "print(f\"Model: TwoLayerNet(p={p}, hidden={hidden_size}, init_scale={init_scale})\")\n",
        "print(f\"Optimizer: PerNeuronScaledSGD(lr={lr}, degree=3)\")\n",
        "print(f\"Training for {epochs} epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history, val_loss_history, param_history, param_save_epochs, final_epoch = train_mod.train(\n",
        "    net,\n",
        "    dataloader,\n",
        "    criterion,\n",
        "    opt,\n",
        "    epochs=epochs,\n",
        "    verbose_interval=max(1, epochs // 10),\n",
        "    save_param_interval=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute theoretical loss plateau levels\n",
        "theory = power.theoretical_loss_levels_1d(tpl)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(loss_history, lw=4)\n",
        "\n",
        "for level in theory[\"levels\"]:\n",
        "    ax.axhline(y=level, color=\"black\", linestyle=\"--\", linewidth=2, zorder=-2)\n",
        "\n",
        "ax.set_xscale(\"log\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Epochs\", fontsize=18)\n",
        "ax.set_ylabel(\"Train Loss\", fontsize=18)\n",
        "ax.set_title(f\"Training loss on $C_{{{p}}}$\", fontsize=20)\n",
        "viz.style_axes(ax)\n",
        "ax.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/cn_loss.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Power Spectrum Over Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute template power for reference lines\n",
        "num_freqs = p // 2 + 1\n",
        "template_ft = np.fft.rfft(tpl)\n",
        "template_power = np.abs(template_ft)[:num_freqs]\n",
        "\n",
        "# Compute output power over time\n",
        "num_points = min(500, len(param_history))\n",
        "steps = np.unique(np.logspace(0, np.log10(max(1, len(param_history) - 1)), num_points, dtype=int))\n",
        "powers_over_time = []\n",
        "\n",
        "for step in steps:\n",
        "    net.load_state_dict(param_history[step])\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = net(X_tensor)\n",
        "        ft = np.fft.rfft(outputs.detach().cpu().numpy(), axis=1)\n",
        "        avg_power = np.mean(np.abs(ft), axis=0)\n",
        "        powers_over_time.append(avg_power)\n",
        "\n",
        "powers_over_time = np.array(powers_over_time)\n",
        "\n",
        "# Plot\n",
        "colors = [\"tab:blue\", \"tab:orange\", \"tab:red\", \"tab:green\", \"tab:brown\", \"tab:purple\"]\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "for k in range(num_freqs):\n",
        "    color = colors[k] if k < len(colors) else f\"C{k}\"\n",
        "    ax.plot(steps, powers_over_time[:, k], color=color, lw=4, label=rf\"$\\xi = {k}$\")\n",
        "    ax.axhline(template_power[k], color=color, linestyle=\"dotted\", linewidth=2, alpha=0.5, zorder=-10)\n",
        "\n",
        "ax.set_xscale(\"log\")\n",
        "ax.set_ylabel(\"Power\", fontsize=18)\n",
        "ax.set_xlabel(\"Epochs\", fontsize=18)\n",
        "ax.set_title(f\"Power spectrum over training on $C_{{{p}}}$\", fontsize=20)\n",
        "ax.legend(fontsize=12, title=\"Frequency\", title_fontsize=14, loc=\"upper left\", labelspacing=0.25)\n",
        "viz.style_axes(ax)\n",
        "ax.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/cn_power_spectrum.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AGF Numerics\n",
        "\n",
        "Compare gradient descent training with the Alternating Gradient Flow (AGF) approximation.\n",
        "AGF decomposes training into alternating utility-maximization and cost-minimization phases,\n",
        "predicting when each Fourier mode activates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class ModsumSubNetwork(nn.Module):\n",
        "    \"\"\"A single neuron of the two-layer network for AGF analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, init_scale):\n",
        "        super().__init__()\n",
        "        assert d_in % 2 == 0\n",
        "        self.p = d_in // 2\n",
        "        self.u = nn.Linear(self.p, 1, bias=False)\n",
        "        self.v = nn.Linear(self.p, 1, bias=False)\n",
        "        self.w = nn.Linear(1, d_out, bias=False)\n",
        "        with torch.no_grad():\n",
        "            self.w.weight.mul_(init_scale)\n",
        "            self.u.weight.mul_(init_scale)\n",
        "            self.v.weight.mul_(init_scale)\n",
        "        self.active = False\n",
        "        self.util_acc = 0\n",
        "        self.c_a = 1 / self.get_norm() - 1\n",
        "        self.normalize()\n",
        "\n",
        "    def get_norm(self):\n",
        "        sqnorm = lambda x: torch.linalg.norm(x.weight) ** 2\n",
        "        return torch.sqrt(sqnorm(self.w) + sqnorm(self.u) + sqnorm(self.v))\n",
        "\n",
        "    def reinitialize(self, u, v, w):\n",
        "        with torch.no_grad():\n",
        "            self.u.weight.copy_(u)\n",
        "            self.v.weight.copy_(v)\n",
        "            self.w.weight.copy_(w)\n",
        "        self.c_a = 1 / self.get_norm() - 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = x[:, : self.p]\n",
        "        x2 = x[:, self.p :]\n",
        "        return self.w((self.u(x1) + self.v(x2)) ** 2)\n",
        "\n",
        "    def normalize(self):\n",
        "        norm = self.get_norm()\n",
        "        with torch.no_grad():\n",
        "            self.w.weight.div_(norm)\n",
        "            self.u.weight.div_(norm)\n",
        "            self.v.weight.div_(norm)\n",
        "\n",
        "    def utility_step(self, x, residual, learning_rate):\n",
        "        f_i = self(x)\n",
        "        util = torch.einsum(\"nd,nd->n\", f_i, residual).mean()\n",
        "        self.util_acc += 3 * learning_rate * util.item()\n",
        "        norm_th = 1 / (1 + self.c_a - self.util_acc)\n",
        "        util.backward()\n",
        "        with torch.no_grad():\n",
        "            self.w.weight += norm_th * learning_rate * self.w.weight.grad\n",
        "            self.u.weight += norm_th * learning_rate * self.u.weight.grad\n",
        "            self.v.weight += norm_th * learning_rate * self.v.weight.grad\n",
        "            self.w.weight.grad.zero_()\n",
        "            self.u.weight.grad.zero_()\n",
        "            self.v.weight.grad.zero_()\n",
        "            self.normalize()\n",
        "\n",
        "\n",
        "class ModsumNetwork(nn.Module):\n",
        "    \"\"\"Network of ModsumSubNetwork neurons for AGF simulation.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_out, init_scale, width=100):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "        self.width = width\n",
        "        neurons = [ModsumSubNetwork(d_in, d_out, init_scale) for _ in range(width)]\n",
        "        self.neurons = nn.ModuleList(neurons)\n",
        "        self.set_mode(\"utilmax\")\n",
        "\n",
        "    def load_init(self, U, V, W):\n",
        "        for i, n in enumerate(self.neurons):\n",
        "            u, v, w = U[i], V[i], W[i][:, None]\n",
        "            n.reinitialize(u, v, w)\n",
        "\n",
        "    def dormant(self):\n",
        "        return [neuron for neuron in self.neurons if not neuron.active]\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        if mode not in [\"utilmax\", \"costmin\"]:\n",
        "            raise ValueError(\"mode must be utilmax or costmin\")\n",
        "        self.mode = mode\n",
        "        for neuron in self.neurons:\n",
        "            grad_on = (mode == \"utilmax\") ^ neuron.active\n",
        "            for param in neuron.parameters():\n",
        "                param.requires_grad = grad_on\n",
        "\n",
        "    def forward(self, x):\n",
        "        active = [n for n in self.neurons if n.active]\n",
        "        if not active:\n",
        "            return torch.zeros(x.shape[0], self.d_out)\n",
        "        outputs = torch.stack([n(x) for n in active], dim=0)\n",
        "        return torch.sum(outputs, dim=0)\n",
        "\n",
        "\n",
        "def train_agf(\n",
        "    X_train, Y_train, init_sz=1e-3, agf_steps=5, from_init=None,\n",
        "    utilmax_lr=1, costmin_lr=1, costmin_maxiter=1e4, loss_thresh=1e-4,\n",
        "):\n",
        "    \"\"\"Run the Alternating Gradient Flow (AGF) approximation.\"\"\"\n",
        "    d_in, d_out = X_train.shape[-1], Y_train.shape[-1]\n",
        "    if from_init:\n",
        "        U, V, W = from_init[\"U\"], from_init[\"V\"], from_init[\"W\"]\n",
        "        width = U.shape[0]\n",
        "        agf_net = ModsumNetwork(d_in, d_out, init_sz, width=width)\n",
        "        agf_net.load_init(U, V, W)\n",
        "    else:\n",
        "        agf_net = ModsumNetwork(d_in, d_out, init_sz, width=agf_steps)\n",
        "    X_train.requires_grad = False\n",
        "\n",
        "    results = {\"t\": [], \"losses\": [], \"pred\": []}\n",
        "\n",
        "    def update_results(t):\n",
        "        results[\"t\"].append(t)\n",
        "        residual = (Y_train - agf_net(X_train)).detach()\n",
        "        results[\"losses\"].append((residual**2).mean().item())\n",
        "        results[\"pred\"].append(agf_net(X_train).detach().cpu().clone())\n",
        "\n",
        "    t = 0\n",
        "    update_results(t)\n",
        "    for _ in tqdm(range(agf_steps)):\n",
        "        residual = (1 / d_out) * 2 * (Y_train - agf_net(X_train))\n",
        "        residual = residual.detach()\n",
        "        iters = 0\n",
        "        mode = \"utilmax\"\n",
        "        while mode == \"utilmax\":\n",
        "            for n in agf_net.neurons:\n",
        "                if n.active:\n",
        "                    continue\n",
        "                n.utility_step(X_train, residual, utilmax_lr)\n",
        "                if n.util_acc > n.c_a:\n",
        "                    n.active = True\n",
        "                    mode = \"costmin\"\n",
        "            iters += 1\n",
        "        agf_net.set_mode(mode)\n",
        "        t += iters\n",
        "\n",
        "        agf_opt = torch.optim.SGD(agf_net.parameters(), lr=costmin_lr, momentum=0.9)\n",
        "        for _ in range(int(costmin_maxiter)):\n",
        "            agf_opt.zero_grad(set_to_none=False)\n",
        "            residual = Y_train - agf_net(X_train)\n",
        "            loss = (residual**2).mean()\n",
        "            loss.backward()\n",
        "            agf_opt.step()\n",
        "        agf_net.set_mode(\"utilmax\")\n",
        "\n",
        "        print(f\"loss: {loss.item():.5f}\")\n",
        "        update_results(t)\n",
        "\n",
        "        if not agf_net.dormant() or loss.item() < loss_thresh:\n",
        "            break\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TEST_MODE:\n",
        "    agf_results = train_agf(\n",
        "        X_tensor, Y_tensor,\n",
        "        init_sz=init_scale,\n",
        "        agf_steps=50,\n",
        "        from_init=param_history[0],\n",
        "        utilmax_lr=0.1,\n",
        "        costmin_lr=0.01,\n",
        "        costmin_maxiter=1e4,\n",
        "        loss_thresh=1e-4,\n",
        "    )\n",
        "else:\n",
        "    agf_results = None\n",
        "    print(\"Skipping AGF in TEST_MODE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss: GD vs AGF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(loss_history, lw=4, label=\"GD\")\n",
        "\n",
        "# Theory plateau levels\n",
        "for level in theory[\"levels\"]:\n",
        "    ax.axhline(y=level, color=\"black\", linestyle=\"--\", linewidth=2, zorder=-2)\n",
        "\n",
        "# AGF overlay\n",
        "if agf_results is not None:\n",
        "    utilmax_lr_val = 0.1\n",
        "    f = utilmax_lr_val / lr\n",
        "    agf_times = agf_results[\"t\"] + [epochs]\n",
        "    agf_losses = agf_results[\"losses\"] + [agf_results[\"losses\"][-1]]\n",
        "    ax.step(f * np.array(agf_times), agf_losses, where=\"post\", lw=2, ls=\"dashed\", color=\"k\", label=\"AGF\")\n",
        "    ax.legend(fontsize=14)\n",
        "\n",
        "ax.set_xscale(\"log\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Epochs\", fontsize=18)\n",
        "ax.set_ylabel(\"Train Loss\", fontsize=18)\n",
        "ax.set_title(f\"GD vs AGF on $C_{{{p}}}$\", fontsize=20)\n",
        "viz.style_axes(ax)\n",
        "ax.grid(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{FIGURES_DIR}/cn_loss_agf.pdf\", bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "group-agf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
