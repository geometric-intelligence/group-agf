{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4265f1a8",
   "metadata": {},
   "source": [
    "# MLP Scaling: $H$ vs $k$\n",
    "\n",
    "Hidden neurons vs sequence length scaling experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05ce99",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# jupyter black formatter\n",
    "%load_ext jupyter_black\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "gitroot_path = subprocess.check_output(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"], universal_newlines=True\n",
    ").strip()\n",
    "\n",
    "os.chdir(gitroot_path)\n",
    "print(\"Working directory: \", os.getcwd())\n",
    "\n",
    "if gitroot_path not in sys.path:\n",
    "    sys.path.insert(0, gitroot_path)\n",
    "print(\"Directory added to path: \", gitroot_path)\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a42140",
   "metadata": {},
   "source": [
    "## Specify experiment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dir = \"/data/facosta/sweeps/sweep_mlp_scaling_20251212_161329\"\n",
    "os.path.exists(sweep_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3acce2",
   "metadata": {},
   "source": [
    "### Final Loss Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af291059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid(sweep_dir: str, k_values: list, hidden_dims: list):\n",
    "    \"\"\"\n",
    "    Load sweep results and organize into a grid for heatmap visualization.\n",
    "\n",
    "    Args:\n",
    "        sweep_dir: Path to the sweep directory\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D numpy array with shape (len(hidden_dims), len(k_values))\n",
    "              containing mean final train losses\n",
    "        std_grid: 2D numpy array with standard deviations (if multiple seeds)\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    # Initialize grids\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    # Load results for each experiment\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                print(f\"Warning: Experiment {exp_name} not found\")\n",
    "                continue\n",
    "\n",
    "            # Load experiment summary\n",
    "            summary_file = exp_dir / \"experiment_summary.yaml\"\n",
    "            if summary_file.exists():\n",
    "                with open(summary_file, \"r\") as f:\n",
    "                    summary = yaml.safe_load(f)\n",
    "\n",
    "                # Get mean train loss\n",
    "                if \"train_loss_stats\" in summary:\n",
    "                    grid[i, j] = summary[\"train_loss_stats\"][\"mean\"]\n",
    "                    std_grid[i, j] = summary[\"train_loss_stats\"][\"std\"]\n",
    "                else:\n",
    "                    print(f\"Warning: No train_loss_stats in {exp_name}\")\n",
    "            else:\n",
    "                print(f\"Warning: No summary file for {exp_name}\")\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e321b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# hidden_dims = [60, 360, 2160, 12960, 77760]\n",
    "hidden_dims = [6, 6**2, 6**3, 6**4, 6**5, 6**6]\n",
    "\n",
    "grid, _ = load_sweep_results_grid(sweep_dir, k_values, hidden_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(grid, aspect=\"auto\", norm=None)\n",
    "\n",
    "# Correct labels: rows = hidden_dims (y), columns = k_values (x)\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "\n",
    "# Set tick labels to show actual values\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "plt.yticks(range(len(hidden_dims)), hidden_dims)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.title(\"Final Train Loss\")\n",
    "\n",
    "plt.colorbar(label=\"Final Train Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf4b02",
   "metadata": {},
   "source": [
    "### Loss Curve Integral Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb980bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid_integral(sweep_dir: str, k_values: list, hidden_dims: list):\n",
    "    \"\"\"\n",
    "    Load sweep results and compute integral of loss curves.\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D array with mean integral of loss curves\n",
    "        std_grid: 2D array with standard deviations across seeds\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                continue\n",
    "\n",
    "            # Collect integrals from all seeds\n",
    "            integrals = []\n",
    "            for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                if loss_file.exists():\n",
    "                    loss_history = np.load(loss_file)\n",
    "                    # Compute integral using trapezoidal rule\n",
    "                    integral = np.trapz(loss_history)\n",
    "                    integrals.append(integral)\n",
    "\n",
    "            if integrals:\n",
    "                grid[i, j] = np.mean(integrals)\n",
    "                std_grid[i, j] = np.std(integrals) if len(integrals) > 1 else 0.0\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5017cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "integral_grid, integral_std = load_sweep_results_grid_integral(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "from matplotlib.colors import LogNorm, SymLogNorm\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(integral_grid, aspect=\"auto\", norm=LogNorm())\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "plt.yticks(range(len(hidden_dims)), hidden_dims)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(label=\"Loss Curve Integral\")\n",
    "plt.title(\"Loss Curve Integral\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9956cf",
   "metadata": {},
   "source": [
    "### Steps to Convergence Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid_convergence(\n",
    "    sweep_dir: str, k_values: list, hidden_dims: list, reduction_threshold: float = 0.99\n",
    "):\n",
    "    \"\"\"\n",
    "    Load sweep results and compute steps to convergence.\n",
    "\n",
    "    Convergence is defined as reaching `reduction_threshold` loss reduction\n",
    "    (e.g., 0.99 = 99% reduction from initial loss).\n",
    "\n",
    "    If convergence is not reached, the grid point is set to NaN (blacked out).\n",
    "\n",
    "    Args:\n",
    "        sweep_dir: Path to the sweep directory\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values\n",
    "        reduction_threshold: Fraction of loss reduction to consider converged\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D array with mean steps to convergence (NaN if didn't converge)\n",
    "        std_grid: 2D array with standard deviations across seeds\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                continue\n",
    "\n",
    "            # Collect convergence steps from all seeds\n",
    "            convergence_steps = []\n",
    "            for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                if loss_file.exists():\n",
    "                    loss_history = np.load(loss_file)\n",
    "                    initial_loss = loss_history[0]\n",
    "\n",
    "                    if initial_loss > 0:\n",
    "                        # Compute reduction at each step\n",
    "                        reductions = 1 - loss_history / initial_loss\n",
    "\n",
    "                        # Find first step where reduction >= threshold\n",
    "                        converged_mask = reductions >= reduction_threshold\n",
    "                        if np.any(converged_mask):\n",
    "                            step = np.argmax(converged_mask)  # First True\n",
    "                            convergence_steps.append(step)\n",
    "                        # else: Never converged - don't add to list\n",
    "\n",
    "            if convergence_steps:\n",
    "                grid[i, j] = np.mean(convergence_steps)\n",
    "                std_grid[i, j] = (\n",
    "                    np.std(convergence_steps) if len(convergence_steps) > 1 else 0.0\n",
    "                )\n",
    "            # else: No seeds converged - grid[i,j] remains NaN (blacked out)\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b083a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_threshold = 0.6\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir, k_values, hidden_dims, reduction_threshold=reduction_threshold\n",
    ")\n",
    "plt.figure(figsize=(10, 6))  # Made slightly wider to accommodate legend\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid, aspect=\"equal\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length ($k$)\")\n",
    "plt.ylabel(\"Hidden Dimension $H$\")\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "\n",
    "# Create y-tick labels with both power notation and actual values\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "y_step = np.minimum(x_step, len(hidden_dims))  # Example: stays within bounds\n",
    "\n",
    "plt.step(\n",
    "    x_step,\n",
    "    y_step,\n",
    "    where=\"post\",\n",
    "    color=\"red\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Theory boundary ($H > 6^{k-1}$)\",\n",
    ")\n",
    "\n",
    "# Place legend outside the plot area (to the right)\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.12), fontsize=12, frameon=True)\n",
    "\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "plt.title(f\"Steps to {reduction_threshold*100}% Convergence (black = did not converge)\")\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130132a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid_spikiness(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dims: list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute fraction of training steps where loss increased (instability).\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D array with mean frac_upward across seeds\n",
    "        std_grid: 2D array with standard deviations\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                continue\n",
    "\n",
    "            frac_upwards = []\n",
    "            for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                if loss_file.exists():\n",
    "                    loss_history = np.load(loss_file)\n",
    "                    log_loss = np.log10(loss_history + 1e-10)\n",
    "                    log_changes = np.diff(log_loss)\n",
    "\n",
    "                    # Fraction of steps where loss went UP\n",
    "                    frac_upward = np.sum(log_changes > 0) / len(log_changes)\n",
    "                    frac_upwards.append(frac_upward)\n",
    "\n",
    "            if frac_upwards:\n",
    "                grid[i, j] = np.mean(frac_upwards)\n",
    "                std_grid[i, j] = np.std(frac_upwards) if len(frac_upwards) > 1 else 0.0\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both convergence and spikiness data\n",
    "reduction_threshold = 0.6  # Adjust as needed\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir, k_values, hidden_dims, reduction_threshold=reduction_threshold\n",
    ")\n",
    "\n",
    "stability_grid, stability_std = load_sweep_results_grid_spikiness(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "# Create a masked version of stability_grid where non-converged runs are NaN\n",
    "stability_grid_masked = stability_grid.copy()\n",
    "stability_grid_masked[np.isnan(conv_grid)] = np.nan  # Mask non-converged runs\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use a colormap with bad values (NaN) shown as black\n",
    "cmap = plt.cm.plasma.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "\n",
    "plt.imshow(stability_grid_masked, aspect=\"equal\", cmap=cmap, vmin=0, vmax=0.5)\n",
    "\n",
    "plt.xlabel(\"Sequence Length ($k$)\")\n",
    "plt.ylabel(\"Hidden Dimension $H$\")\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "\n",
    "# Create y-tick labels with both power notation and actual values\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "y_step = np.minimum(x_step, len(hidden_dims))  # Example: stays within bounds\n",
    "\n",
    "plt.step(\n",
    "    x_step,\n",
    "    y_step,\n",
    "    where=\"post\",\n",
    "    color=\"red\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Theory boundary ($H > 6^{k-1}$)\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "\n",
    "plt.colorbar(label=\"Fraction of Upward Steps\")\n",
    "plt.title(\n",
    "    f\"Training Instability\\n(black = did not converge)\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "n_converged = np.sum(~np.isnan(conv_grid))\n",
    "n_not_converged = np.sum(np.isnan(conv_grid))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Converged runs: {n_converged} ({100*n_converged/conv_grid.size:.1f}%)\")\n",
    "print(\n",
    "    f\"Did not converge (black): {n_not_converged} ({100*n_not_converged/conv_grid.size:.1f}%)\"\n",
    ")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a228687",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_grid, stability_std = load_sweep_results_grid_spikiness(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "# Create side-by-side subplots\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ========== RIGHT PANEL: SPIKINESS ==========\n",
    "# Use a different colormap - plasma, magma, or RdYlGn_r work well\n",
    "plt.imshow(stability_grid, aspect=\"equal\", cmap=\"plasma\", vmin=0, vmax=0.5)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Sequence Length ($k$)\")\n",
    "plt.ylabel(\"Hidden Dimension $H$\")\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "\n",
    "# Create y-tick labels with both power notation and actual values\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "y_step = np.minimum(x_step, len(hidden_dims))  # Example: stays within bounds\n",
    "\n",
    "plt.step(\n",
    "    x_step,\n",
    "    y_step,\n",
    "    where=\"post\",\n",
    "    color=\"red\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Theory boundary ($H > 6^{k-1}$)\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "\n",
    "plt.colorbar(label=\"Fraction of Upward Steps\")\n",
    "# cbar2.ax.axhline(0.3, color=\"white\", linewidth=2, linestyle=\"--\")  # Mark threshold\n",
    "plt.title(\n",
    "    \"Training Instability\\n(higher = more unstable)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both metrics\n",
    "reduction_threshold = 0.6\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir, k_values, hidden_dims, reduction_threshold=reduction_threshold\n",
    ")\n",
    "stability_grid, stability_std = load_sweep_results_grid_spikiness(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "# Create binary mask: 1 if converged AND spiky, 0 otherwise\n",
    "spikiness_threshold = 0.3  # Adjust this threshold as needed\n",
    "\n",
    "converged_and_spiky = np.zeros_like(conv_grid)\n",
    "for i in range(len(hidden_dims)):\n",
    "    for j in range(len(k_values)):\n",
    "        converged = not np.isnan(conv_grid[i, j])\n",
    "        spiky = stability_grid[i, j] > spikiness_threshold\n",
    "\n",
    "        if converged and spiky:\n",
    "            converged_and_spiky[i, j] = 1.0\n",
    "        else:\n",
    "            converged_and_spiky[i, j] = np.nan  # Will show as white\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "\n",
    "# Custom colormap: white for NaN, red for 1\n",
    "cmap = plt.cm.Reds.copy()\n",
    "cmap.set_bad(color=\"white\")\n",
    "\n",
    "im = plt.imshow(converged_and_spiky, aspect=\"equal\", cmap=cmap, vmin=0, vmax=1)\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\", fontsize=12)\n",
    "plt.ylabel(\"Hidden Dimension\", fontsize=12)\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add theory boundary\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "y_step = np.minimum(x_step, len(hidden_dims))\n",
    "plt.step(\n",
    "    x_step,\n",
    "    y_step,\n",
    "    where=\"post\",\n",
    "    color=\"blue\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Theory boundary\",\n",
    ")\n",
    "plt.legend(loc=\"upper left\", fontsize=10, frameon=True)\n",
    "\n",
    "# Add text annotations showing spikiness values in colored cells\n",
    "for i in range(len(hidden_dims)):\n",
    "    for j in range(len(k_values)):\n",
    "        if converged_and_spiky[i, j] == 1.0:\n",
    "            spikiness_val = stability_grid[i, j]\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{spikiness_val:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=9,\n",
    "                color=\"white\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "plt.colorbar(im, label=\"Converged & Spiky\", ticks=[0, 1])\n",
    "plt.title(\n",
    "    f\"Converged BUT Spiky Runs\\n(threshold: frac_upward > {spikiness_threshold})\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "n_converged_spiky = np.sum(converged_and_spiky == 1.0)\n",
    "n_converged_total = np.sum(~np.isnan(conv_grid))\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY: Converged & Spiky Runs\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Spikiness threshold: {spikiness_threshold} (frac_upward)\")\n",
    "print(f\"Converged & spiky:   {n_converged_spiky}\")\n",
    "print(f\"Total converged:     {n_converged_total}\")\n",
    "print(f\"Percentage:          {100*n_converged_spiky/n_converged_total:.1f}%\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both metrics\n",
    "reduction_threshold = 0.6\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir, k_values, hidden_dims, reduction_threshold=reduction_threshold\n",
    ")\n",
    "stability_grid, stability_std = load_sweep_results_grid_spikiness(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "spikiness_threshold = 0.3\n",
    "\n",
    "# Create modified grid for plotting\n",
    "# Strategy: Use a modified colormap and data array\n",
    "plot_grid = conv_grid.copy()\n",
    "\n",
    "# Create mask for spiky converged runs\n",
    "spiky_mask = np.zeros_like(conv_grid, dtype=bool)\n",
    "for i in range(len(hidden_dims)):\n",
    "    for j in range(len(k_values)):\n",
    "        converged = not np.isnan(conv_grid[i, j])\n",
    "        spiky = stability_grid[i, j] > spikiness_threshold\n",
    "        if converged and spiky:\n",
    "            spiky_mask[i, j] = True\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6.5))\n",
    "\n",
    "# First: plot convergence grid with viridis_r (will handle black for NaN)\n",
    "cmap_conv = plt.cm.viridis_r.copy()\n",
    "cmap_conv.set_bad(color=\"black\")\n",
    "im = ax.imshow(plot_grid, aspect=\"equal\", cmap=cmap_conv, norm=LogNorm())\n",
    "\n",
    "# Second: overlay red patches for spiky converged runs\n",
    "for i in range(len(hidden_dims)):\n",
    "    for j in range(len(k_values)):\n",
    "        if spiky_mask[i, j]:\n",
    "            # Draw red square\n",
    "            rect = plt.Rectangle(\n",
    "                (j - 0.5, i - 0.5),\n",
    "                1,\n",
    "                1,\n",
    "                facecolor=\"red\",\n",
    "                edgecolor=\"darkred\",\n",
    "                linewidth=2,\n",
    "                alpha=0.9,\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # # Add convergence value in white text\n",
    "            # conv_val = conv_grid[i, j]\n",
    "            # ax.text(\n",
    "            #     j,\n",
    "            #     i,\n",
    "            #     f\"{int(conv_val)}\",\n",
    "            #     ha=\"center\",\n",
    "            #     va=\"center\",\n",
    "            #     fontsize=8,\n",
    "            #     color=\"white\",\n",
    "            #     fontweight=\"bold\",\n",
    "            # )\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel(\"Sequence Length (k)\", fontsize=12)\n",
    "ax.set_ylabel(\"Hidden Dimension\", fontsize=12)\n",
    "ax.set_xticks(range(len(k_values)))\n",
    "ax.set_xticklabels(k_values)\n",
    "\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "ax.set_yticks(range(len(hidden_dims)))\n",
    "ax.set_yticklabels(ytick_labels)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add theory boundary\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "y_step = np.minimum(x_step, len(hidden_dims))\n",
    "ax.step(\n",
    "    x_step,\n",
    "    y_step,\n",
    "    where=\"post\",\n",
    "    color=\"cyan\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=\"Theory boundary\",\n",
    ")\n",
    "\n",
    "# Custom legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"black\", label=\"Did not converge\"),\n",
    "    Patch(\n",
    "        facecolor=\"red\",\n",
    "        edgecolor=\"darkred\",\n",
    "        linewidth=2,\n",
    "        label=f\"Spiky (frac_up > {spikiness_threshold})\",\n",
    "    ),\n",
    "    Patch(facecolor=\"yellow\", label=\"Smooth convergence\"),\n",
    "    plt.Line2D(\n",
    "        [0], [0], color=\"cyan\", linewidth=3, linestyle=\"--\", label=\"Theory boundary\"\n",
    "    ),\n",
    "]\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.12),\n",
    "    fontsize=10,\n",
    "    frameon=True,\n",
    "    ncol=4,\n",
    ")\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "ax.set_title(\"Convergence Speed & Spikiness Combined\", fontsize=13, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "n_not_converged = np.sum(np.isnan(conv_grid))\n",
    "n_converged_spiky = np.sum(spiky_mask)\n",
    "n_converged_smooth = np.sum(~np.isnan(conv_grid)) - n_converged_spiky\n",
    "total = conv_grid.size\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\n",
    "    f\"Did not converge (black): {n_not_converged:3d} ({100*n_not_converged/total:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Spiky converged (red):    {n_converged_spiky:3d} ({100*n_converged_spiky/total:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Smooth converged (color): {n_converged_smooth:3d} ({100*n_converged_smooth/total:.1f}%)\"\n",
    ")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cbbf5",
   "metadata": {},
   "source": [
    "### Curve plot: Convergence steps vs Sequence Length $k$ for different hidden dimensions\n",
    "- x-axis: sequence length $k$\n",
    "- y-axis: number of steps to convergence\n",
    "- different curves for different hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_vs_k(\n",
    "    conv_grid,\n",
    "    k_values,\n",
    "    hidden_dims,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    "    reduction_threshold=0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot steps to convergence vs sequence length k for different hidden dimensions.\n",
    "\n",
    "    Args:\n",
    "        conv_grid: 2D array (len(hidden_dims), len(k_values)) with convergence steps\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values\n",
    "        save_path: Where to save the plot\n",
    "        show: Whether to display the plot\n",
    "        log_x: Whether to use log scale for x-axis\n",
    "        log_y: Whether to use log scale for y-axis\n",
    "        reduction_threshold: Threshold used for convergence\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Use a nice sequential colormap for different widths\n",
    "    colors = plt.cm.plasma(np.linspace(0.15, 0.95, len(hidden_dims)))\n",
    "\n",
    "    for i, (h, color) in enumerate(zip(hidden_dims, colors)):\n",
    "        # Extract convergence steps for this hidden dim across all k values\n",
    "        steps_for_h = conv_grid[i, :]\n",
    "\n",
    "        # Only plot converged points\n",
    "        converged_mask = ~np.isnan(steps_for_h)\n",
    "        k_converged = np.array(k_values)[converged_mask]\n",
    "        steps_converged = steps_for_h[converged_mask]\n",
    "\n",
    "        if len(steps_converged) > 0:\n",
    "            # Plot with line and markers\n",
    "            ax.plot(\n",
    "                k_converged,\n",
    "                steps_converged,\n",
    "                color=color,\n",
    "                marker=\"o\",\n",
    "                markersize=7,\n",
    "                linewidth=2.5,\n",
    "                label=f\"h={h:,}\",\n",
    "                markeredgewidth=0.5,\n",
    "                markeredgecolor=\"white\",\n",
    "            )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Sequence Length ($k$)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Steps to Convergence\", fontsize=14)\n",
    "    ax.set_title(\n",
    "        f\"Steps to {reduction_threshold*100}% Convergence vs Sequence Length $k$\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "    if log_y:\n",
    "        ax.set_yscale(\"log\")\n",
    "    if log_x:\n",
    "        ax.set_xscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=11, framealpha=0.9, loc=\"best\")\n",
    "\n",
    "    # Make k values discrete on x-axis\n",
    "    ax.set_xticks(k_values)\n",
    "    ax.set_xticklabels(k_values)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "reduction_threshold = 0.9\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir,\n",
    "    k_values,\n",
    "    hidden_dims,\n",
    "    reduction_threshold=reduction_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "plot_convergence_vs_k(\n",
    "    conv_grid=conv_grid,\n",
    "    k_values=k_values,\n",
    "    hidden_dims=hidden_dims,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=False,\n",
    "    log_y=True,\n",
    "    reduction_threshold=reduction_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e814b11",
   "metadata": {},
   "source": [
    "### Curve plot : Normalized Convergence Steps vs Sequence Length for different hidden dimensions\n",
    "- x-axis: sequence length\n",
    "- y-axis: normalized convergence steps ($\\text{steps} / |G|^k$)\n",
    "- different curves for different hidden dimensions $H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_vs_k_normalized(\n",
    "    conv_grid,\n",
    "    k_values,\n",
    "    hidden_dims,\n",
    "    p: int = 100,  # Vocabulary size\n",
    "    batch_size: int = 1000,  # Batch size used in training\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=False,\n",
    "    log_y=True,\n",
    "    reduction_threshold=0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot fraction of data space seen to convergence vs sequence length k.\n",
    "\n",
    "    Normalizes steps to convergence by the data space size (p^k) to show\n",
    "    what fraction of the data space needs to be seen for convergence.\n",
    "\n",
    "    Args:\n",
    "        conv_grid: 2D array (len(hidden_dims), len(k_values)) with convergence steps\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values\n",
    "        p: Vocabulary size (data space per token)\n",
    "        batch_size: Batch size used during training\n",
    "        save_path: Where to save the plot\n",
    "        show: Whether to display the plot\n",
    "        log_x: Whether to use log scale for x-axis\n",
    "        log_y: Whether to use log scale for y-axis\n",
    "        reduction_threshold: Threshold used for convergence\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Use a nice sequential colormap for different widths\n",
    "    colors = plt.cm.plasma(np.linspace(0.15, 0.95, len(hidden_dims)))\n",
    "\n",
    "    for i, (h, color) in enumerate(zip(hidden_dims, colors)):\n",
    "        # Extract convergence steps for this hidden dim across all k values\n",
    "        steps_for_h = conv_grid[i, :]\n",
    "\n",
    "        # Only plot converged points\n",
    "        converged_mask = ~np.isnan(steps_for_h)\n",
    "        k_converged = np.array(k_values)[converged_mask]\n",
    "        steps_converged = steps_for_h[converged_mask]\n",
    "\n",
    "        if len(steps_converged) > 0:\n",
    "            # Normalize by data space size for each k\n",
    "            # samples_seen = steps * batch_size\n",
    "            # fraction = samples_seen / p^k\n",
    "            fractions = []\n",
    "            for k_val, steps_val in zip(k_converged, steps_converged):\n",
    "                data_space_size = p**k_val\n",
    "                samples_seen = steps_val * batch_size\n",
    "                fraction = samples_seen / data_space_size\n",
    "                fractions.append(fraction)\n",
    "\n",
    "            # Plot with line and markers\n",
    "            ax.plot(\n",
    "                k_converged,\n",
    "                fractions,\n",
    "                color=color,\n",
    "                marker=\"o\",\n",
    "                markersize=7,\n",
    "                linewidth=2.5,\n",
    "                label=f\"h={h:,}\",\n",
    "                markeredgewidth=0.5,\n",
    "                markeredgecolor=\"white\",\n",
    "            )\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Sequence Length (k)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Data points seen / $p^k$ to convergence\", fontsize=14)\n",
    "    ax.set_title(\n",
    "        f\"Data Efficiency to {reduction_threshold*100}% Convergence\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    if log_y:\n",
    "        ax.set_yscale(\"log\")\n",
    "    if log_x:\n",
    "        ax.set_xscale(\"log\")\n",
    "    else:\n",
    "        # Make k values discrete on x-axis\n",
    "        ax.set_xticks(k_values)\n",
    "        ax.set_xticklabels(k_values)\n",
    "\n",
    "    ax.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=11, framealpha=0.9, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "reduction_threshold = 0.9\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir,\n",
    "    k_values,\n",
    "    hidden_dims,\n",
    "    reduction_threshold=reduction_threshold,\n",
    ")\n",
    "\n",
    "\n",
    "plot_convergence_vs_k_normalized(\n",
    "    conv_grid=conv_grid,\n",
    "    k_values=k_values,\n",
    "    hidden_dims=hidden_dims,\n",
    "    p=10,  # Your vocabulary size\n",
    "    batch_size=1000,  # Your batch size\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=False,\n",
    "    log_y=True,\n",
    "    reduction_threshold=reduction_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f758af",
   "metadata": {},
   "source": [
    "### Curve plot: Loss vs Training Steps for different sequence lengths, fixed hidden dimension\n",
    "- x-axis: # training steps\n",
    "- y-axis: training loss\n",
    "- different curves for different sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e809783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_loss_curves_fixed_width(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dim: int = 77760,\n",
    "    seed: int = 0,\n",
    "    save_path: str = None,\n",
    "    show: bool = True,\n",
    "    log_x: bool = True,\n",
    "    log_y: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot loss curves for different sequence lengths k with fixed hidden dimension.\n",
    "\n",
    "    Args:\n",
    "        sweep_dir: Path to sweep directory\n",
    "        k_values: List of k values to plot (e.g., [2, 3, 4, 5, 6, 7, 8])\n",
    "        hidden_dim: Fixed hidden dimension (default: 77760)\n",
    "        seed: Which seed to plot (default: 0)\n",
    "        save_path: Where to save the plot\n",
    "        show: Whether to display the plot\n",
    "        log_x: Whether to use log scale for x-axis\n",
    "        log_y: Whether to use log scale for y-axis\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Use a nice sequential colormap (plasma, magma, cividis, YlOrRd, etc.)\n",
    "    colors = plt.cm.plasma(\n",
    "        np.linspace(0.15, 0.95, len(k_values))\n",
    "    )  # Avoid too light/dark\n",
    "\n",
    "    for k, color in zip(k_values, colors):\n",
    "        run_dir = sweep_path / f\"k{k}_h{hidden_dim}\" / f\"seed_{seed}\"\n",
    "        loss_file = run_dir / \"train_loss_history.npy\"\n",
    "\n",
    "        if not loss_file.exists():\n",
    "            print(f\"Warning: No data found for k={k}, h={hidden_dim}\")\n",
    "            continue\n",
    "\n",
    "        # Load loss history\n",
    "        loss_history = np.load(loss_file)\n",
    "        steps = np.arange(len(loss_history))\n",
    "\n",
    "        # Plot\n",
    "        ax.plot(steps, loss_history, color=color, lw=2.5, label=f\"k={k}\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Training Steps\", fontsize=14)\n",
    "    ax.set_ylabel(\"Training Loss\", fontsize=14)\n",
    "    ax.set_title(f\"Loss vs Training Steps (h={hidden_dim:,})\", fontsize=16)\n",
    "    if log_x:\n",
    "        ax.set_xscale(\"log\")\n",
    "    if log_y:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=11, framealpha=0.9, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_loss_curves_fixed_width(\n",
    "    sweep_dir=sweep_dir,\n",
    "    k_values=[2, 3, 4, 5, 6, 7, 8],\n",
    "    hidden_dim=6**2,\n",
    "    seed=0,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spikiness_metrics_upward_only(loss_history):\n",
    "    \"\"\"\n",
    "    Compute spikiness focusing ONLY on upward jumps (loss increases).\n",
    "\n",
    "    This separates:\n",
    "    - Fast learning (large downward jumps) = STABLE\n",
    "    - Instability (upward jumps) = UNSTABLE/SPIKY\n",
    "    \"\"\"\n",
    "    log_loss = np.log10(loss_history + 1e-10)\n",
    "    log_changes = np.diff(log_loss)  # Can be positive or negative\n",
    "\n",
    "    # Separate upward (bad) from downward (good)\n",
    "    upward_spikes = log_changes[log_changes > 0]  # Loss INCREASES\n",
    "    downward_drops = log_changes[log_changes < 0]  # Loss DECREASES\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Count how many steps are upward vs downward\n",
    "    metrics[\"n_upward\"] = len(upward_spikes)\n",
    "    metrics[\"n_downward\"] = len(downward_drops)\n",
    "    metrics[\"frac_upward\"] = (\n",
    "        len(upward_spikes) / len(log_changes) if len(log_changes) > 0 else 0\n",
    "    )\n",
    "\n",
    "    if len(upward_spikes) > 0:\n",
    "        # Metrics based ONLY on upward spikes (instability)\n",
    "        metrics[\"upward_p95\"] = np.percentile(upward_spikes, 95)\n",
    "        metrics[\"upward_p999\"] = np.percentile(upward_spikes, 99.9)\n",
    "        metrics[\"upward_max\"] = np.max(upward_spikes)\n",
    "        metrics[\"upward_mean\"] = np.mean(upward_spikes)\n",
    "        metrics[\"upward_std\"] = np.std(upward_spikes)\n",
    "    else:\n",
    "        # Perfectly monotonic decrease (never went up!)\n",
    "        metrics[\"upward_p95\"] = 0.0\n",
    "        metrics[\"upward_p999\"] = 0.0\n",
    "        metrics[\"upward_max\"] = 0.0\n",
    "        metrics[\"upward_mean\"] = 0.0\n",
    "        metrics[\"upward_std\"] = 0.0\n",
    "\n",
    "    if len(downward_drops) > 0:\n",
    "        # For reference: how fast is it learning?\n",
    "        metrics[\"downward_p95\"] = np.percentile(\n",
    "            np.abs(downward_drops), 95\n",
    "        )  # Large drops = fast\n",
    "        metrics[\"downward_mean\"] = np.mean(np.abs(downward_drops))\n",
    "    else:\n",
    "        metrics[\"downward_p95\"] = 0.0\n",
    "        metrics[\"downward_mean\"] = 0.0\n",
    "\n",
    "    # Ratio: upward spikes vs downward progress\n",
    "    if metrics[\"downward_mean\"] > 0:\n",
    "        metrics[\"spike_to_progress_ratio\"] = (\n",
    "            metrics[\"upward_mean\"] / metrics[\"downward_mean\"]\n",
    "        )\n",
    "    else:\n",
    "        metrics[\"spike_to_progress_ratio\"] = (\n",
    "            np.inf if metrics[\"upward_mean\"] > 0 else 0.0\n",
    "        )\n",
    "\n",
    "    # Late-stage upward spikes (last 20%)\n",
    "    cutoff = int(0.8 * len(log_changes))\n",
    "    late_changes = log_changes[cutoff:]\n",
    "    late_upward = late_changes[late_changes > 0]\n",
    "\n",
    "    if len(late_upward) > 0:\n",
    "        metrics[\"late_upward_p95\"] = np.percentile(late_upward, 95)\n",
    "        metrics[\"late_upward_max\"] = np.max(late_upward)\n",
    "    else:\n",
    "        metrics[\"late_upward_p95\"] = 0.0\n",
    "        metrics[\"late_upward_max\"] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_loss_curves_with_upward_metrics(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dim: int = 36,\n",
    "    seed: int = 0,\n",
    "    save_path: str = None,\n",
    "    show: bool = True,\n",
    "    log_x: bool = True,\n",
    "    log_y: bool = True,\n",
    "):\n",
    "    \"\"\"Plot loss curves with metrics focused on upward spikes only.\"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = plt.cm.plasma(np.linspace(0.15, 0.95, len(k_values)))\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    for k, color in zip(k_values, colors):\n",
    "        run_dir = sweep_path / f\"k{k}_h{hidden_dim}\" / f\"seed_{seed}\"\n",
    "        loss_file = run_dir / \"train_loss_history.npy\"\n",
    "\n",
    "        if not loss_file.exists():\n",
    "            print(f\"Warning: No data found for k={k}, h={hidden_dim}\")\n",
    "            continue\n",
    "\n",
    "        loss_history = np.load(loss_file)\n",
    "        steps = np.arange(len(loss_history))\n",
    "\n",
    "        ax.plot(steps, loss_history, color=color, lw=2.5, label=f\"k={k}\")\n",
    "\n",
    "        # Compute upward-only metrics\n",
    "        metrics = compute_spikiness_metrics_upward_only(loss_history)\n",
    "        metrics[\"k\"] = k\n",
    "        metrics[\"n_steps\"] = len(loss_history)\n",
    "        metrics_data.append(metrics)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Training Steps\", fontsize=14)\n",
    "    ax.set_ylabel(\"Training Loss\", fontsize=14)\n",
    "    ax.set_title(f\"Loss vs Training Steps (h={hidden_dim:,})\", fontsize=16)\n",
    "    if log_x:\n",
    "        ax.set_xscale(\"log\")\n",
    "    if log_y:\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=11, framealpha=0.9, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    # Print metrics\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    col_order = [\n",
    "        \"k\",\n",
    "        \"n_steps\",\n",
    "        \"frac_upward\",\n",
    "        \"upward_p95\",\n",
    "        \"upward_p999\",\n",
    "        \"upward_max\",\n",
    "        \"late_upward_p95\",\n",
    "        \"spike_to_progress_ratio\",\n",
    "        \"downward_p95\",\n",
    "    ]\n",
    "    df = df[col_order]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"UPWARD SPIKE METRICS (h={hidden_dim})\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\nMetric Definitions:\")\n",
    "    print(\"  frac_upward              : Fraction of steps where loss INCREASED\")\n",
    "    print(\"  upward_p95               : 95th percentile of upward jumps (instability)\")\n",
    "    print(\"  upward_p999              : 99.9th percentile of upward jumps\")\n",
    "    print(\"  upward_max               : Worst upward spike\")\n",
    "    print(\"  late_upward_p95          : 95th percentile of upward jumps in last 20%\")\n",
    "    print(\n",
    "        \"  spike_to_progress_ratio  : Mean upward / mean downward (higher = more unstable)\"\n",
    "    )\n",
    "    print(\n",
    "        \"  downward_p95             : 95th percentile of downward jumps (learning speed)\"\n",
    "    )\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return fig, ax, df\n",
    "\n",
    "\n",
    "# Call it:\n",
    "fig, ax, metrics_df = plot_loss_curves_with_upward_metrics(\n",
    "    sweep_dir=sweep_dir,\n",
    "    k_values=[2, 3, 4, 5, 6, 7, 8],\n",
    "    hidden_dim=6**6,\n",
    "    seed=0,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid_spikiness(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dims: list,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute fraction of training steps where loss increased (instability).\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D array with mean frac_upward across seeds\n",
    "        std_grid: 2D array with standard deviations\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                continue\n",
    "\n",
    "            frac_upwards = []\n",
    "            for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                if loss_file.exists():\n",
    "                    loss_history = np.load(loss_file)\n",
    "                    log_loss = np.log10(loss_history + 1e-10)\n",
    "                    log_changes = np.diff(log_loss)\n",
    "\n",
    "                    # Fraction of steps where loss went UP\n",
    "                    frac_upward = np.sum(log_changes > 0) / len(log_changes)\n",
    "                    frac_upwards.append(frac_upward)\n",
    "\n",
    "            if frac_upwards:\n",
    "                grid[i, j] = np.mean(frac_upwards)\n",
    "                std_grid[i, j] = np.std(frac_upwards) if len(frac_upwards) > 1 else 0.0\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d858f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stability grid\n",
    "stability_grid, stability_std = load_sweep_results_grid_spikiness(\n",
    "    sweep_dir, k_values, hidden_dims\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "plt.imshow(stability_grid, aspect=\"equal\", cmap=\"viridis\")  # , norm=LogNorm())\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "ytick_labels = [f\"$6^{i+1}$ ({val:,})\" for i, val in enumerate(hidden_dims)]\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels)\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.colorbar(label=\"Training Spikiness\")\n",
    "plt.title(\"Training Spikiness\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e758e",
   "metadata": {},
   "source": [
    "# Varying group size (num frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sweep_results_grid_convergence_3d(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dims: list,\n",
    "    num_frequencies: int,\n",
    "    reduction_threshold: float = 0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load sweep results and compute steps to convergence for 3D sweeps over k, h, and f.\n",
    "\n",
    "    This function is designed for sweeps that include num_frequencies as a parameter,\n",
    "    using directory naming format: k{k}_h{h}_f{f}\n",
    "\n",
    "    Convergence is defined as reaching `reduction_threshold` loss reduction\n",
    "    (e.g., 0.99 = 99% reduction from initial loss).\n",
    "\n",
    "    If convergence is not reached, the grid point is set to NaN (blacked out).\n",
    "\n",
    "    Args:\n",
    "        sweep_dir: Path to the sweep directory\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values\n",
    "        num_frequencies: Number of frequencies (f parameter)\n",
    "        reduction_threshold: Fraction of loss reduction to consider converged\n",
    "\n",
    "    Returns:\n",
    "        grid: 2D array with mean steps to convergence (NaN if didn't converge)\n",
    "        std_grid: 2D array with standard deviations across seeds\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "    std_grid = np.full((len(hidden_dims), len(k_values)), np.nan)\n",
    "\n",
    "    for i, h in enumerate(hidden_dims):\n",
    "        for j, k in enumerate(k_values):\n",
    "            exp_name = f\"k{k}_h{h}_f{num_frequencies}\"\n",
    "            exp_dir = sweep_path / exp_name\n",
    "\n",
    "            if not exp_dir.exists():\n",
    "                continue\n",
    "\n",
    "            # Collect convergence steps from all seeds\n",
    "            convergence_steps = []\n",
    "            for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                if loss_file.exists():\n",
    "                    loss_history = np.load(loss_file)\n",
    "                    initial_loss = loss_history[0]\n",
    "\n",
    "                    if initial_loss > 0:\n",
    "                        # Compute reduction at each step\n",
    "                        reductions = 1 - loss_history / initial_loss\n",
    "\n",
    "                        # Find first step where reduction >= threshold\n",
    "                        converged_mask = reductions >= reduction_threshold\n",
    "                        if np.any(converged_mask):\n",
    "                            step = np.argmax(converged_mask)  # First True\n",
    "                            convergence_steps.append(step)\n",
    "                        # else: Never converged - don't add to list\n",
    "\n",
    "            if convergence_steps:\n",
    "                grid[i, j] = np.mean(convergence_steps)\n",
    "                std_grid[i, j] = (\n",
    "                    np.std(convergence_steps) if len(convergence_steps) > 1 else 0.0\n",
    "                )\n",
    "            # else: No seeds converged - grid[i,j] remains NaN (blacked out)\n",
    "\n",
    "    return grid, std_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d1ac0",
   "metadata": {},
   "source": [
    "## num_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the new 3D sweep (k, h, f)\n",
    "new_sweep_dir = \"/home/facosta/group-agf/sweeps/sweep_mlp_scaling_20251212_172318\"\n",
    "\n",
    "# Define parameter values for the new sweep\n",
    "k_values_new = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims_new = [6, 36, 216, 1296, 7776, 46656]\n",
    "num_frequencies = 2  # Set this to the frequency value you want to visualize\n",
    "\n",
    "# Load convergence data for a specific frequency\n",
    "reduction_threshold = 0.5\n",
    "conv_grid_new, conv_std_new = load_sweep_results_grid_convergence_3d(\n",
    "    new_sweep_dir,\n",
    "    k_values_new,\n",
    "    hidden_dims_new,\n",
    "    reduction_threshold=reduction_threshold,\n",
    "    num_frequencies=num_frequencies,\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid_new, aspect=\"equal\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "plt.xticks(range(len(k_values_new)), k_values_new)\n",
    "plt.yticks(range(len(hidden_dims_new)), hidden_dims_new)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "plt.title(f\"Steps to Convergence (f={num_frequencies}, black = did not converge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b06b2",
   "metadata": {},
   "source": [
    "## num_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e86647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the new 3D sweep (k, h, f)\n",
    "new_sweep_dir = \"/home/facosta/group-agf/sweeps/sweep_mlp_scaling_20251212_172318\"\n",
    "\n",
    "# Define parameter values for the new sweep\n",
    "k_values_new = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims_new = [6, 36, 216, 1296, 7776, 46656]\n",
    "num_frequencies = 3  # Set this to the frequency value you want to visualize\n",
    "\n",
    "# Load convergence data for a specific frequency\n",
    "reduction_threshold = 0.5\n",
    "conv_grid_new, conv_std_new = load_sweep_results_grid_convergence_3d(\n",
    "    new_sweep_dir,\n",
    "    k_values_new,\n",
    "    hidden_dims_new,\n",
    "    reduction_threshold=reduction_threshold,\n",
    "    num_frequencies=num_frequencies,\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid_new, aspect=\"equal\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "plt.xticks(range(len(k_values_new)), k_values_new)\n",
    "plt.yticks(range(len(hidden_dims_new)), hidden_dims_new)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "plt.title(f\"Steps to Convergence (f={num_frequencies}, black = did not converge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4e9c7",
   "metadata": {},
   "source": [
    "## num_freq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the new 3D sweep (k, h, f)\n",
    "new_sweep_dir = \"/home/facosta/group-agf/sweeps/sweep_mlp_scaling_20251212_172318\"\n",
    "\n",
    "# Define parameter values for the new sweep\n",
    "k_values_new = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims_new = [6, 36, 216, 1296, 7776, 46656]\n",
    "num_frequencies = 4  # Set this to the frequency value you want to visualize\n",
    "\n",
    "# Load convergence data for a specific frequency\n",
    "reduction_threshold = 0.5\n",
    "conv_grid_new, conv_std_new = load_sweep_results_grid_convergence_3d(\n",
    "    new_sweep_dir,\n",
    "    k_values_new,\n",
    "    hidden_dims_new,\n",
    "    reduction_threshold=reduction_threshold,\n",
    "    num_frequencies=num_frequencies,\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid_new, aspect=\"equal\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "plt.xticks(range(len(k_values_new)), k_values_new)\n",
    "plt.yticks(range(len(hidden_dims_new)), hidden_dims_new)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "plt.title(f\"Steps to Convergence (f={num_frequencies}, black = did not converge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ae86c",
   "metadata": {},
   "source": [
    "## num_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce18ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using the new 3D sweep (k, h, f)\n",
    "new_sweep_dir = \"/home/facosta/group-agf/sweeps/sweep_mlp_scaling_20251212_172318\"\n",
    "\n",
    "# Define parameter values for the new sweep\n",
    "k_values_new = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims_new = [6, 36, 216, 1296, 7776, 46656]\n",
    "num_frequencies = 5  # Set this to the frequency value you want to visualize\n",
    "\n",
    "# Load convergence data for a specific frequency\n",
    "reduction_threshold = 0.5\n",
    "conv_grid_new, conv_std_new = load_sweep_results_grid_convergence_3d(\n",
    "    new_sweep_dir,\n",
    "    k_values_new,\n",
    "    hidden_dims_new,\n",
    "    reduction_threshold=reduction_threshold,\n",
    "    num_frequencies=num_frequencies,\n",
    ")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid_new, aspect=\"equal\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\")\n",
    "plt.ylabel(\"Hidden Dimension\")\n",
    "plt.xticks(range(len(k_values_new)), k_values_new)\n",
    "plt.yticks(range(len(hidden_dims_new)), hidden_dims_new)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100}% Convergence\")\n",
    "plt.title(f\"Steps to Convergence (f={num_frequencies}, black = did not converge)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8585f79",
   "metadata": {},
   "source": [
    "### Grid plot: Convergence vs k for different num_frequencies, across different hidden dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c43fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_vs_k_grid_by_frequency(\n",
    "    sweep_dir: str,\n",
    "    k_values: list,\n",
    "    hidden_dims: list,\n",
    "    num_frequencies_values: list,\n",
    "    reduction_threshold: float = 0.99,\n",
    "    figsize=(18, 12),\n",
    "    log_x=False,\n",
    "    log_y=True,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a grid of plots showing convergence vs k for different frequencies.\n",
    "\n",
    "    Each subplot corresponds to a different hidden dimension.\n",
    "    Within each subplot, different curves represent different num_frequencies values.\n",
    "\n",
    "    Args:\n",
    "        sweep_dir: Path to the sweep directory\n",
    "        k_values: List of k (sequence length) values\n",
    "        hidden_dims: List of hidden dimension values (one subplot per hidden dim)\n",
    "        num_frequencies_values: List of num_frequencies values to compare\n",
    "        reduction_threshold: Threshold for convergence definition\n",
    "        figsize: Figure size tuple\n",
    "        log_x: Whether to use log scale for x-axis\n",
    "        log_y: Whether to use log scale for y-axis\n",
    "        save_path: Where to save the plot\n",
    "        show: Whether to display the plot\n",
    "\n",
    "    Returns:\n",
    "        fig, axes: Matplotlib figure and axes objects\n",
    "    \"\"\"\n",
    "    sweep_path = Path(sweep_dir)\n",
    "\n",
    "    # Determine grid layout (2x3 or 3x2 based on number of hidden dims)\n",
    "    n_plots = len(hidden_dims)\n",
    "    if n_plots == 6:\n",
    "        nrows, ncols = 2, 3\n",
    "    elif n_plots == 4:\n",
    "        nrows, ncols = 2, 2\n",
    "    else:\n",
    "        # General case: aim for squarish layout\n",
    "        ncols = int(np.ceil(np.sqrt(n_plots)))\n",
    "        nrows = int(np.ceil(n_plots / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes_flat = axes.flatten() if n_plots > 1 else [axes]\n",
    "\n",
    "    # Use a nice colormap for different frequencies\n",
    "    colors = plt.cm.viridis(np.linspace(0.15, 0.85, len(num_frequencies_values)))\n",
    "\n",
    "    for idx, h in enumerate(hidden_dims):\n",
    "        ax = axes_flat[idx]\n",
    "\n",
    "        # For each frequency, plot convergence vs k\n",
    "        for f_idx, num_freq in enumerate(num_frequencies_values):\n",
    "            convergence_steps_for_k = []\n",
    "            k_values_converged = []\n",
    "\n",
    "            for k in k_values:\n",
    "                exp_name = f\"k{k}_h{h}_f{num_freq}\"\n",
    "                exp_dir = sweep_path / exp_name\n",
    "\n",
    "                if not exp_dir.exists():\n",
    "                    continue\n",
    "\n",
    "                # Collect convergence steps from all seeds\n",
    "                convergence_steps = []\n",
    "                for seed_dir in exp_dir.glob(\"seed_*\"):\n",
    "                    loss_file = seed_dir / \"train_loss_history.npy\"\n",
    "                    if loss_file.exists():\n",
    "                        loss_history = np.load(loss_file)\n",
    "                        initial_loss = loss_history[0]\n",
    "\n",
    "                        if initial_loss > 0:\n",
    "                            reductions = 1 - loss_history / initial_loss\n",
    "                            converged_mask = reductions >= reduction_threshold\n",
    "                            if np.any(converged_mask):\n",
    "                                step = np.argmax(converged_mask)\n",
    "                                convergence_steps.append(step)\n",
    "\n",
    "                # Take mean across seeds if any converged\n",
    "                if convergence_steps:\n",
    "                    k_values_converged.append(k)\n",
    "                    convergence_steps_for_k.append(np.mean(convergence_steps))\n",
    "\n",
    "            # Plot this frequency's curve\n",
    "            if len(k_values_converged) > 0:\n",
    "                ax.plot(\n",
    "                    k_values_converged,\n",
    "                    convergence_steps_for_k,\n",
    "                    color=colors[f_idx],\n",
    "                    marker=\"o\",\n",
    "                    markersize=6,\n",
    "                    linewidth=2,\n",
    "                    label=f\"f={num_freq}\",\n",
    "                    markeredgewidth=0.5,\n",
    "                    markeredgecolor=\"white\",\n",
    "                )\n",
    "\n",
    "        # Formatting for this subplot\n",
    "        ax.set_xlabel(\"Sequence Length (k)\", fontsize=11)\n",
    "        ax.set_ylabel(\"Steps to Convergence\", fontsize=11)\n",
    "        ax.set_title(f\"h = {h:,}\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "        if log_y:\n",
    "            ax.set_yscale(\"log\")\n",
    "        if log_x:\n",
    "            ax.set_xscale(\"log\")\n",
    "        else:\n",
    "            # Make k values discrete on x-axis\n",
    "            ax.set_xticks(k_values)\n",
    "            ax.set_xticklabels(k_values)\n",
    "\n",
    "        ax.grid(True, alpha=0.3, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "        ax.legend(fontsize=9, framealpha=0.9, loc=\"best\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for idx in range(n_plots, len(axes_flat)):\n",
    "        axes_flat[idx].axis(\"off\")\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle(\n",
    "        f\"Convergence vs Sequence Length by Number of Frequencies\\n\"\n",
    "        f\"({reduction_threshold*100:.0f}% Loss Reduction Threshold)\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.995,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd00a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Create grid plot\n",
    "new_sweep_dir = \"/home/facosta/group-agf/sweeps/sweep_mlp_scaling_20251212_172318\"\n",
    "\n",
    "k_values_new = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims_new = [6, 36, 216, 1296, 7776, 46656]\n",
    "num_frequencies_values = [2, 3, 4, 5]  # All frequency values to compare\n",
    "\n",
    "plot_convergence_vs_k_grid_by_frequency(\n",
    "    sweep_dir=new_sweep_dir,\n",
    "    k_values=k_values_new,\n",
    "    hidden_dims=hidden_dims_new,\n",
    "    num_frequencies_values=num_frequencies_values,\n",
    "    reduction_threshold=0.5,\n",
    "    figsize=(18, 12),\n",
    "    log_x=False,\n",
    "    log_y=True,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c094f2",
   "metadata": {},
   "source": [
    "## p=2 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep directory and parameters\n",
    "sweep_dir = \"/home/facosta/group-agf/sweeps/p2_scaling_sweep_20251215_205347\"\n",
    "\n",
    "# Parameters from p2_scaling_sweep.yaml\n",
    "k_values = [2, 3, 4, 5, 6, 7, 8]\n",
    "hidden_dims = [\n",
    "    4,\n",
    "    8,\n",
    "    16,\n",
    "    32,\n",
    "    64,\n",
    "    128,\n",
    "    256,\n",
    "    512,\n",
    "    1024,\n",
    "    2048,\n",
    "    4096,\n",
    "    8192,\n",
    "    16384,\n",
    "    32768,\n",
    "    65536,\n",
    "]\n",
    "\n",
    "# Load convergence data\n",
    "reduction_threshold = 0.6  # 90% loss reduction\n",
    "conv_grid, conv_std = load_sweep_results_grid_convergence(\n",
    "    sweep_dir, k_values, hidden_dims, reduction_threshold=reduction_threshold\n",
    ")\n",
    "\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.viridis_r.copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "plt.imshow(conv_grid, aspect=\"auto\", cmap=cmap, norm=LogNorm())\n",
    "\n",
    "plt.xlabel(\"Sequence Length (k)\", fontsize=12)\n",
    "plt.ylabel(\"Hidden Dimension (h)\", fontsize=12)\n",
    "plt.xticks(range(len(k_values)), k_values)\n",
    "\n",
    "# Create y-tick labels with both power notation and actual values for larger dims\n",
    "ytick_labels = []\n",
    "for h in hidden_dims:\n",
    "    if h >= 1024:\n",
    "        power = int(np.log2(h))\n",
    "        ytick_labels.append(f\"$2^{{{power}}}$ ({h:,})\")\n",
    "    else:\n",
    "        ytick_labels.append(f\"{h}\")\n",
    "\n",
    "plt.yticks(range(len(hidden_dims)), ytick_labels, fontsize=9)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add theoretical boundary line (h > p^(k-1), where p=2)\n",
    "# For p=2: boundary at h = 2^(k-1), so h=1,2,4,8,16,32,64\n",
    "x_step = np.arange(len(k_values)) - 0.5\n",
    "# Find y index where h = 2^(k-1) for each k\n",
    "y_boundary = []\n",
    "for i, k in enumerate(k_values):\n",
    "    boundary_h = 2 ** (k - 1)\n",
    "    # Find closest hidden_dim index\n",
    "    try:\n",
    "        y_idx = hidden_dims.index(boundary_h)\n",
    "    except ValueError:\n",
    "        # If exact match not found, find closest\n",
    "        y_idx = np.argmin(np.abs(np.array(hidden_dims) - boundary_h))\n",
    "    y_boundary.append(y_idx)\n",
    "\n",
    "plt.step(\n",
    "    x_step,\n",
    "    y_boundary,\n",
    "    where=\"post\",\n",
    "    color=\"red\",\n",
    "    linewidth=3,\n",
    "    linestyle=\"--\",\n",
    "    label=r\"Theory boundary ($h > 2^{k-1}$)\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=11, frameon=True)\n",
    "plt.colorbar(label=f\"Steps to {reduction_threshold*100:.0f}% Convergence\")\n",
    "plt.title(\n",
    "    f\"p=2 Scaling: Steps to {reduction_threshold*100:.0f}% Convergence\\n(black = did not converge)\",\n",
    "    fontsize=13,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "n_not_converged = np.sum(np.isnan(conv_grid))\n",
    "n_converged = np.sum(~np.isnan(conv_grid))\n",
    "total = conv_grid.size\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CONVERGENCE SUMMARY (p=2 scaling)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Converged:         {n_converged:3d} ({100*n_converged/total:.1f}%)\")\n",
    "print(f\"Did not converge:  {n_not_converged:3d} ({100*n_not_converged/total:.1f}%)\")\n",
    "print(f\"Total experiments: {total:3d}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e10a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group-agf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
