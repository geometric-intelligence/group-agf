{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d11caf-0971-4324-b63b-819b714a9c3c",
   "metadata": {},
   "source": [
    "# Binary Group Composition with $C_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f249f1-6985-4c73-86cd-04e1adac3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd05577-db56-4d0a-bb93-1d0b48cecaf6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19bd1ad-9e8f-4720-b317-afe13fafae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(p):\n",
    "    \"\"\"One-hot encode an integer value in R^p.\"\"\"\n",
    "    vec = np.zeros(p)\n",
    "    vec[0] = 1\n",
    "    return vec\n",
    "\n",
    "def generate_template(p, magnitude, exponent):\n",
    "    weight = magnitude * np.power(np.arange(1, p), -exponent)  # Power-law singular values\n",
    "    template = np.ones(p)  # Base term (DC component)\n",
    "    for freq in range(1, p):\n",
    "        template += weight[freq-1] * np.cos(np.arange(p) * freq / p * 2 * np.pi)\n",
    "    return template / p\n",
    "\n",
    "def generate_fixed_template(p):\n",
    "    # Generate template array from Fourier spectrum\n",
    "    spectrum = np.zeros(p, dtype=complex)\n",
    "    \n",
    "    # Set only three frequencies with specific amplitudes\n",
    "    spectrum[1] = 12.5 # Positive frequency\n",
    "    spectrum[-1] = 12.5  # Negative frequency (conjugate)\n",
    "    spectrum[2] = 10 # Positive frequency\n",
    "    spectrum[-2] = 10  # Negative frequency (conjugate)\n",
    "    spectrum[3] = 7.5 # Second frequency\n",
    "    spectrum[-3] =  7.5  # Its conjugate\n",
    "    spectrum[4] = 5 # Second frequency\n",
    "    spectrum[-4] =  5  # Its conjugate\n",
    "    spectrum[5] = 2.5  # Third frequency \n",
    "    spectrum[-5] = 2.5  # Its conjugate\n",
    "    \n",
    "    # Generate signal from spectrum\n",
    "    template = np.fft.ifft(spectrum).real\n",
    "\n",
    "    return template\n",
    "\n",
    "def ModularAdditionDataset(p, template):\n",
    "    # Initialize data arrays\n",
    "    X = np.zeros((p * p, 2, p))  # Shape: (p^2, 2, p)\n",
    "    Y = np.zeros((p * p, p))     # Shape: (p^2, p)\n",
    "    \n",
    "    # Generate the dataset\n",
    "    idx = 0\n",
    "    for a in range(p):\n",
    "        for b in range(p):\n",
    "            q = (a + b) % p  # a + b mod p\n",
    "            X[idx, 0, :] = np.roll(template, a)\n",
    "            X[idx, 1, :] = np.roll(template, b)\n",
    "            Y[idx, :] = np.roll(template, q)\n",
    "            idx += 1\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ecbbd-ceaf-4bef-af4a-13a22fa70063",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf22b7d-49e7-445b-8742-2e75cd1fa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, p, hidden_size, nonlinearity='square', init_scale=1.0, output_scale=1.0):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.p = p\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.init_scale = init_scale\n",
    "        self.output_scale = output_scale\n",
    "        \n",
    "        # Initialize parameters \n",
    "        self.U = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(2 * p))  # First p elements\n",
    "        self.V = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(2 * p))  # Second p elements\n",
    "        self.W = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(p)) # Second layer weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # First layer (linear and combined)\n",
    "        x1 = x[:, :self.p] @ self.U.T\n",
    "        x2 = x[:, self.p:] @ self.V.T\n",
    "        x_combined = x1 + x2\n",
    "\n",
    "        # Apply nonlinearity activation\n",
    "        if self.nonlinearity == 'relu':\n",
    "            x_combined = torch.relu(x_combined)\n",
    "        elif self.nonlinearity == 'square':\n",
    "            x_combined = x_combined**2\n",
    "        elif self.nonlinearity == 'linear':\n",
    "            x_combined = x_combined\n",
    "        elif self.nonlinearity == 'tanh':\n",
    "            x_combined = torch.tanh(x_combined)\n",
    "        elif self.nonlinearity == 'gelu':\n",
    "            gelu = torch.nn.GELU()\n",
    "            x_combined = gelu(x_combined)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid nonlinearity '{self.nonlinearity}' provided.\")\n",
    "\n",
    "        # Second layer (linear)\n",
    "        x_out = x_combined @ self.W\n",
    "\n",
    "        # Feature learning scaling\n",
    "        x_out *= self.output_scale\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7336b-5c6e-48af-a357-2b2c877f6168",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ba87b-9607-4a4a-9b00-00c15adb2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PerNeuronScaledSGD(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Per-neuron scaled SGD optimizer that exploits model homogeneity.\n",
    "    \n",
    "    Learning rate scaling per neuron i:\n",
    "        eta_i = lr * ||theta_i||^(2-degree)\n",
    "    \n",
    "    where:\n",
    "        - theta_i comprises all parameters associated with neuron i\n",
    "        - degree is the degree of homogeneity of the model\n",
    "\n",
    "    For SequentialMLP with sequence length k:\n",
    "        - theta_i = (W_in[i, :], W_out[:, i])\n",
    "        - degree = k+1 (activation is x^k, one more layer for W_out)\n",
    "\n",
    "    For TwoLayerNet:\n",
    "        - theta_i = (U[i, :], V[i, :], W[i, :])\n",
    "        - degree:\n",
    "            * nonlinearity == 'square' -> 3\n",
    "            * otherwise default 2 (unless explicitly provided)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        model, \n",
    "        lr=1.0, \n",
    "        degree=None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: SequentialMLP, TwoLayerNet, or compatible model\n",
    "            lr: base learning rate\n",
    "            degree: degree of homogeneity (exponent for norm-based scaling)\n",
    "                    If None, inferred from model:\n",
    "                      - SequentialMLP: uses k+1 where k is sequence length\n",
    "                      - TwoLayerNet:\n",
    "                            * 'square'  -> 3\n",
    "                            * otherwise -> 2\n",
    "                      - Default: 2\n",
    "        \"\"\"\n",
    "        model_type = type(model).__name__\n",
    "\n",
    "        # Infer degree of homogeneity from model if not provided\n",
    "        if degree is None:\n",
    "            if hasattr(model, 'k'):  # SequentialMLP-style\n",
    "                degree = model.k + 1\n",
    "            elif model_type == 'TwoLayerNet':\n",
    "                nl = getattr(model, 'nonlinearity', None)\n",
    "                if nl == 'square':\n",
    "                    degree = 3\n",
    "                else:\n",
    "                    # For relu/linear/tanh/gelu or unknown, fall back\n",
    "                    degree = 2\n",
    "            else:\n",
    "                # Default for quadratic-ish models\n",
    "                degree = 2\n",
    "        \n",
    "        # Get model parameters\n",
    "        params = list(model.parameters())\n",
    "        \n",
    "        super().__init__(\n",
    "            [{'params': params, 'model': model, 'model_type': model_type}], \n",
    "            dict(lr=lr, degree=degree)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        group = self.param_groups[0]\n",
    "        model = group['model']\n",
    "        lr = group['lr']\n",
    "        degree = group['degree']\n",
    "        model_type = group['model_type']\n",
    "        \n",
    "        if model_type == 'SequentialMLP':\n",
    "            # SequentialMLP: W_in (d, k*p), W_out (p, d)\n",
    "            W_in = model.W_in\n",
    "            W_out = model.W_out\n",
    "            g_in = W_in.grad\n",
    "            g_out = W_out.grad\n",
    "            \n",
    "            if g_in is None or g_out is None:\n",
    "                return\n",
    "            \n",
    "            # Per-neuron norms: theta_i = (W_in[i, :], W_out[:, i])\n",
    "            u2 = (W_in**2).sum(dim=1)    # (d,)\n",
    "            w2 = (W_out**2).sum(dim=0)   # (d,)\n",
    "            theta_norm = torch.sqrt(u2 + w2 + 1e-12)  # (d,)\n",
    "            \n",
    "            # Scale = ||theta_i||^(2-degree)\n",
    "            scale = theta_norm.pow(2 - degree)\n",
    "            \n",
    "            # Scale each neuron's gradients\n",
    "            g_in.mul_(scale.view(-1, 1))\n",
    "            g_out.mul_(scale.view(1, -1))\n",
    "            \n",
    "            # SGD update\n",
    "            W_in.add_(g_in, alpha=-lr)\n",
    "            W_out.add_(g_out, alpha=-lr)\n",
    "\n",
    "        elif model_type == 'TwoLayerNet':\n",
    "            # TwoLayerNet: U (d, p), V (d, p), W (d, p)\n",
    "            U = model.U\n",
    "            V = model.V\n",
    "            W = model.W\n",
    "\n",
    "            g_U = U.grad\n",
    "            g_V = V.grad\n",
    "            g_W = W.grad\n",
    "\n",
    "            if g_U is None or g_V is None or g_W is None:\n",
    "                return\n",
    "\n",
    "            # Per-neuron norms: theta_i = (U[i, :], V[i, :], W[i, :])\n",
    "            u2 = (U**2).sum(dim=1)   # (hidden_size,)\n",
    "            v2 = (V**2).sum(dim=1)   # (hidden_size,)\n",
    "            w2 = (W**2).sum(dim=1)   # (hidden_size,)\n",
    "            theta_norm = torch.sqrt(u2 + v2 + w2 + 1e-12)  # (hidden_size,)\n",
    "\n",
    "            # Scale = ||theta_i||^(2-degree)\n",
    "            scale = theta_norm.pow(2 - degree)            # (hidden_size,)\n",
    "\n",
    "            # Scale each neuron's gradients\n",
    "            scale_view = scale.view(-1, 1)  # (hidden_size, 1)\n",
    "            g_U.mul_(scale_view)\n",
    "            g_V.mul_(scale_view)\n",
    "            g_W.mul_(scale_view)\n",
    "\n",
    "            # SGD update\n",
    "            U.add_(g_U, alpha=-lr)\n",
    "            V.add_(g_V, alpha=-lr)\n",
    "            W.add_(g_W, alpha=-lr)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"PerNeuronScaledSGD: Unsupported model structure with {model_type}\")\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035f81c-e877-4655-8640-4e4c3d323af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten input for FC layers\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the largest value (class)\n",
    "            _, true_labels = torch.max(labels, 1)  # Get the true class from the one-hot encoding\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, epochs=100, verbose_interval=10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_history = []  # List to store loss values\n",
    "    accuracy_history = []\n",
    "    param_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten input for FC layers\n",
    "\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Append the average loss for the epoch to loss_history\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        # Append the accuracy\n",
    "        model.eval()\n",
    "        accuracy = test_accuracy(model, dataloader)\n",
    "        accuracy_history.append(accuracy)\n",
    "        model.train()\n",
    "\n",
    "        # Save current model parameters\n",
    "        current_params = {\n",
    "            \"U\": model.U.detach().cpu().clone(),\n",
    "            \"V\": model.V.detach().cpu().clone(),\n",
    "            \"W\": model.W.detach().cpu().clone()\n",
    "        }\n",
    "        param_history.append(current_params)\n",
    "\n",
    "        # Print verbose information every `verbose_interval` epochs\n",
    "        if (epoch + 1) % verbose_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return loss_history, accuracy_history, param_history # Return loss history for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86c4f6-83a6-4465-abf0-7d104432cc9c",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e2d10-9550-4fd4-adb7-168a27fda1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_axes(ax, numyticks=5, numxticks=5, labelsize=24):\n",
    "    # Y-axis ticks\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", bottom=True, top=False,\n",
    "                   labelbottom=True, left=True, right=False,\n",
    "                   labelleft=True, direction='out', length=7, width=1.5, pad=8, labelsize=labelsize)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=numyticks))\n",
    "    \n",
    "    # X-axis ticks\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=True, top=False,\n",
    "                   labelbottom=True, left=True, right=False,\n",
    "                   labelleft=True, direction='out', length=7, width=1.5, pad=8, labelsize=labelsize)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=numxticks))\n",
    "\n",
    "    # Scientific notation formatting\n",
    "    if ax.get_yscale() == 'linear':\n",
    "        ax.ticklabel_format(style='sci', axis='y', scilimits=(-2, 2))\n",
    "    if ax.get_xscale() == 'linear':\n",
    "        ax.ticklabel_format(style='sci', axis='x', scilimits=(-2, 2))\n",
    "\n",
    "    ax.xaxis.offsetText.set_fontsize(20)\n",
    "    ax.grid()\n",
    "\n",
    "    # Customize spines\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    for spine in [\"left\", \"bottom\"]:\n",
    "        ax.spines[spine].set_linewidth(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20989d96-f34f-4be7-a0f9-4b92fb7f235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power(points):\n",
    "    p = len(points)\n",
    "    num_coefficients = (p // 2) + 1\n",
    "    \n",
    "    # Perform FFT and calculate power spectrum\n",
    "    ft = np.fft.fft(points) # Could consider using np.fft.rfft which is designed for real valued input.\n",
    "    power = np.abs(ft[:num_coefficients])**2 / p\n",
    "    \n",
    "    # Double power for frequencies strictly between 0 and Nyquist (Nyquist is not doubled if p is even)\n",
    "    if p % 2 == 0:  # p is even, Nyquist frequency at index num_coefficients - 1\n",
    "        power[1:num_coefficients - 1] *= 2\n",
    "    else:  # p is odd, no Nyquist frequency\n",
    "        power[1:] *= 2\n",
    "\n",
    "    # Confirm the power sum approximates the squared norm of points\n",
    "    total_power = np.sum(power)\n",
    "    norm_squared = np.linalg.norm(points)**2\n",
    "    if not np.isclose(total_power, norm_squared, rtol=1e-3):\n",
    "        print(f\"Warning: Total power {total_power:.3f} does not match norm squared {norm_squared:.3f}\")\n",
    "\n",
    "    return np.arange(num_coefficients), power\n",
    "\n",
    "def interpolate(ax, points, color, continuous, alpha=1.0):\n",
    "    p = len(points)\n",
    "    if continuous:\n",
    "        # Perform Fourier Transform\n",
    "        ft = np.fft.fft(points)\n",
    "        \n",
    "        # Keep only non-negative frequencies (first half + Nyquist if p is even)\n",
    "        num_coefficients = (p // 2) + 1\n",
    "        ft = ft[:num_coefficients]  # Truncate to keep non-negative frequencies\n",
    "        \n",
    "        # Create a dense set of x-values for smooth interpolation\n",
    "        xs = np.linspace(0, p, 10 * p)  # 10 times more points than the original for smoothness\n",
    "        curr_val = np.zeros(xs.shape, dtype=complex)\n",
    "        \n",
    "        # Use only non-negative frequencies for interpolation\n",
    "        for freq in range(num_coefficients):\n",
    "            theta = np.angle(ft[freq])\n",
    "            r = np.abs(ft[freq]) / p\n",
    "            # Double amplitude except for DC (freq = 0) and Nyquist (freq = p / 2, when p is even)\n",
    "            if freq > 0 and (freq < p / 2 or p % 2 != 0):\n",
    "                r *= 2\n",
    "            curr_val += r * np.exp(1j * ((2 * np.pi * freq * xs / p) + theta))\n",
    "\n",
    "        # Plot the real part (since output is real-valued)\n",
    "        ax.plot(xs, curr_val.real, color=color, alpha=alpha)\n",
    "    else:\n",
    "        ax.plot(np.arange(p), points, color=color, alpha=alpha)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99dae27-f8fe-403a-b70f-0bcaf818cbe7",
   "metadata": {},
   "source": [
    "## Gradient Descent Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd15c5a-5745-41ba-b015-48e403160c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  # or any integer you like\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "\n",
    "# TEST_MODE: Reduce p and hidden_size for faster automated testing\n",
    "import os\n",
    "TEST_MODE = os.environ.get(\"NOTEBOOK_TEST_MODE\", \"0\") == \"1\"\n",
    "\n",
    "# Data Generation using the new function\n",
    "p = 11  # Keep same value in TEST_MODE to avoid index errors  # Modulus (reduced in test mode)\n",
    "\n",
    "# Get base vector\n",
    "# template = one_hot(p)\n",
    "template = generate_fixed_template(p)\n",
    "\n",
    "# Mean center template\n",
    "template -= np.mean(template)\n",
    "\n",
    "# Generate dataset using numpy\n",
    "X, Y = ModularAdditionDataset(p, template)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).view(-1, 2 * p)  # Flatten input (num_samples, 2*p)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)  # Targets (num_samples, p)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "hidden_size = 20 if TEST_MODE else 200  # Reduced in test mode\n",
    "model = TwoLayerNet(p=p, hidden_size=hidden_size, nonlinearity='square', init_scale=1e-5, output_scale=1e0)\n",
    "\n",
    "# Create loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Construct optimizer\n",
    "lr = 0.01\n",
    "optimizer = PerNeuronScaledSGD(model, lr=lr, degree=3)  # explicit\n",
    "\n",
    "# Train the model\n",
    "# TEST_MODE: Set to reduce epochs for automated testing\n",
    "import os\n",
    "TEST_MODE = os.environ.get(\"NOTEBOOK_TEST_MODE\", \"0\") == \"1\"\n",
    "epochs = 2 if TEST_MODE else 50001\n",
    "loss_history, accuracy_history, param_history = train(model, dataloader, loss, optimizer, epochs=epochs, verbose_interval=max(1, epochs//10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48aebc-a439-405a-a057-3f5c24cca91a",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46febe-abb5-459a-bb06-a18a26afb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.plot(list(loss_history), lw=7)\n",
    "\n",
    "# === Compute power spectrum of template ===\n",
    "freq, power = get_power(template)\n",
    "valid = power > 1e-20\n",
    "freq, power = freq[valid], power[valid]\n",
    "sorted_idx = np.argsort(-power)\n",
    "freq, power = freq[sorted_idx], power[sorted_idx]\n",
    "\n",
    "alpha_values = [np.sum(power[k:]) for k in range(len(power))]\n",
    "coef = 1 / p\n",
    "for k, alpha in enumerate(alpha_values):\n",
    "    ax.axhline(y=coef * alpha, color='black', linestyle='--', linewidth=2, zorder=-2)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1e-2, 10)\n",
    "ax.set_xlabel('Epochs', fontsize=24)\n",
    "ax.set_ylabel('Train Loss', fontsize=24)\n",
    "\n",
    "style_axes(ax)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss-without-lines.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"loss-without-lines.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b851e7-6256-43cd-b9f3-aca38db04917",
   "metadata": {},
   "source": [
    "## Power Spectrum of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b25ca9-6339-49dd-9d45-577a51798a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETTINGS ===\n",
    "p = Y_tensor.shape[1]\n",
    "num_freqs = p // 2 + 1\n",
    "mom = 0.9\n",
    "\n",
    "# Compute template power spectrum\n",
    "template_ft = np.fft.rfft(template)\n",
    "template_power = np.abs(template_ft)[:num_freqs]\n",
    "\n",
    "# === Compute power spectrum of template ===\n",
    "freq, power = get_power(template)\n",
    "valid = power > 1e-20\n",
    "freq, power = freq[valid], power[valid]\n",
    "sorted_idx = np.argsort(-power)\n",
    "freq, power = freq[sorted_idx], power[sorted_idx]\n",
    "\n",
    "# === Theory lines ===\n",
    "alpha_values = [np.sum(power[k:]) for k in range(len(power))]\n",
    "coef = 1 / p\n",
    "theta0 = np.sqrt(2) * model.init_scale\n",
    "uMax = [np.sqrt(2 * p / 27) * (p * power[k] / 2)**(3/2) / p**2 for k in range(len(power))]\n",
    "tau_values = [(1 / theta0 - 1) / (3 * uMax[k]) for k in range(len(uMax))]\n",
    "step_size = 2 * coef * lr / (1 - mom)\n",
    "\n",
    "\n",
    "# Color settings\n",
    "cmap = plt.colormaps.get_cmap('tab20').resampled(num_freqs)\n",
    "manual_colors = {\n",
    "    0: 'tab:blue',\n",
    "    1: 'tab:orange',\n",
    "    2: 'tab:red',\n",
    "    3: 'tab:green',\n",
    "    4: 'tab:brown',\n",
    "    5: 'tab:purple',\n",
    "}\n",
    "colors = [manual_colors.get(i, cmap(i)) for i in range(num_freqs)]\n",
    "\n",
    "# Compute output power over time (GD)\n",
    "num_points = 1000\n",
    "steps = np.unique(np.logspace(0, np.log10(len(param_history) - 1), num_points, dtype=int))\n",
    "powers_over_time = []\n",
    "\n",
    "for step in steps:\n",
    "    model.load_state_dict(param_history[step])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        ft = np.fft.rfft(outputs.detach().cpu().numpy(), axis=1)\n",
    "        avg_power = np.mean(np.abs(ft), axis=0)\n",
    "        powers_over_time.append(avg_power)\n",
    "\n",
    "powers_over_time = np.array(powers_over_time)  # shape: (steps, freqs)\n",
    "\n",
    "# === PLOTTING ===\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for k in range(num_freqs):\n",
    "    color = colors[k]\n",
    "    label = fr\"$\\xi = {k}$\"# if k in [1, 3, 5] else None\n",
    "    ax.plot(steps, powers_over_time[:, k], color=color, lw=5, label=label)\n",
    "    label_agf = 'AGF' if k == 10 else None\n",
    "    ax.axhline(template_power[k], color=color, linestyle='dotted', linewidth=2, alpha=0.5, zorder=-10)\n",
    "\n",
    "\n",
    "# Labeling and formatting\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Power\", fontsize=24)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=24)\n",
    "ax.legend(fontsize=14, title=\"Frequency\", title_fontsize=16, loc='upper left', labelspacing=0.25)\n",
    "style_axes(ax)\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fourier_power_only.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(\"fourier_power_only.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2c971-d9f1-41e6-b8eb-4e467496ccfd",
   "metadata": {},
   "source": [
    "## Plot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333d1ab-1501-434f-86d2-82c10bb58f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Choose the template (example: first row of Y_tensor)\n",
    "# -------------------------------------------------------------------\n",
    "template = Y_tensor[0].detach().cpu().numpy()  # shape (p,)\n",
    "p = template.shape[0]\n",
    "x = np.arange(p)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Figure 1: Template as black bar plot\n",
    "# -------------------------------------------------------------------\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "ax1.bar(x, template, color=\"black\")\n",
    "\n",
    "ax1.set_xlabel(\"Index\", fontsize=14)\n",
    "ax1.set_ylabel(\"Template value\", fontsize=14)\n",
    "style_axes(ax1)\n",
    "ax1.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig1.savefig(\"template_bar.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Figure 2: Fourier magnitude bar plot with conjugate-pair coloring\n",
    "# -------------------------------------------------------------------\n",
    "# Compute Fourier transform and magnitude\n",
    "fft_template = np.fft.fft(template)\n",
    "fft_mag = np.abs(fft_template)\n",
    "freqs = np.arange(p)\n",
    "\n",
    "# Number of *frequency groups* accounting for conjugate symmetry:\n",
    "# groups: 0, 1, ..., floor(p/2)\n",
    "num_groups = p // 2 + 1\n",
    "\n",
    "# Color settings\n",
    "cmap = plt.colormaps.get_cmap('tab20').resampled(num_groups)\n",
    "manual_colors = {\n",
    "    0: 'tab:blue',\n",
    "    1: 'tab:orange',\n",
    "    2: 'tab:red',\n",
    "    3: 'tab:green',\n",
    "    4: 'tab:brown',\n",
    "    5: 'tab:purple',\n",
    "}\n",
    "group_colors = [manual_colors.get(i, cmap(i)) for i in range(num_groups)]\n",
    "\n",
    "# Assign each k a color based on its conjugate-symmetry group\n",
    "bar_colors = []\n",
    "for k in range(p):\n",
    "    # group index: k and p-k share the same group\n",
    "    g = k if k <= p // 2 else p - k\n",
    "    bar_colors.append(group_colors[g])\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "ax2.bar(freqs, fft_mag, color=bar_colors)\n",
    "\n",
    "ax2.set_xlabel(\"Frequency index $k$\", fontsize=14)\n",
    "ax2.set_ylabel(r\"$|\\hat{t}[k]|$\", fontsize=14)\n",
    "style_axes(ax2)\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig2.savefig(\"template_fft_bar.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb1adb-66ed-44a1-9b6a-ef5dcc6dbe11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group-agf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
