{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular Addition"
   ],
   "id": "51d11caf-0971-4324-b63b-819b714a9c3c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import MaxNLocator"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "80f249f1-6985-4c73-86cd-04e1adac3e8d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ],
   "id": "9fd05577-db56-4d0a-bb93-1d0b48cecaf6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def one_hot(p):\n",
    "    \"\"\"One-hot encode an integer value in R^p.\"\"\"\n",
    "    vec = np.zeros(p)\n",
    "    vec[0] = 1\n",
    "    return vec\n",
    "\n",
    "def generate_template(p, magnitude, exponent):\n",
    "    weight = magnitude * np.power(np.arange(1, p), -exponent)  # Power-law singular values\n",
    "    template = np.ones(p)  # Base term (DC component)\n",
    "    for freq in range(1, p):\n",
    "        template += weight[freq-1] * np.cos(np.arange(p) * freq / p * 2 * np.pi)\n",
    "    return template / p\n",
    "\n",
    "def generate_fixed_template(p):\n",
    "    # Generate template array from Fourier spectrum\n",
    "    spectrum = np.zeros(p, dtype=complex)\n",
    "    \n",
    "    # Set only three frequencies with specific amplitudes\n",
    "    spectrum[1] = 10 # Positive frequency\n",
    "    spectrum[-1] = 10  # Negative frequency (conjugate)\n",
    "    spectrum[3] = 5 # Second frequency\n",
    "    spectrum[-3] =  5  # Its conjugate\n",
    "    spectrum[5] = 2.5  # Third frequency \n",
    "    spectrum[-5] = 2.5  # Its conjugate\n",
    "    \n",
    "    # Generate signal from spectrum\n",
    "    template = np.fft.ifft(spectrum).real\n",
    "\n",
    "    return template\n",
    "\n",
    "def ModularAdditionDataset(p, template):\n",
    "    # Initialize data arrays\n",
    "    X = np.zeros((p * p, 2, p))  # Shape: (p^2, 2, p)\n",
    "    Y = np.zeros((p * p, p))     # Shape: (p^2, p)\n",
    "    \n",
    "    # Generate the dataset\n",
    "    idx = 0\n",
    "    for a in range(p):\n",
    "        for b in range(p):\n",
    "            q = (a + b) % p  # a + b mod p\n",
    "            X[idx, 0, :] = np.roll(template, a)\n",
    "            X[idx, 1, :] = np.roll(template, b)\n",
    "            Y[idx, :] = np.roll(template, q)\n",
    "            idx += 1\n",
    "            \n",
    "    return X, Y"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f19bd1ad-9e8f-4720-b317-afe13fafae88"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ],
   "id": "7a0ecbbd-ceaf-4bef-af4a-13a22fa70063"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, p, hidden_size, nonlinearity='square', init_scale=1.0, output_scale=1.0):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.p = p\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.init_scale = init_scale\n",
    "        self.output_scale = output_scale\n",
    "        \n",
    "        # Initialize parameters \n",
    "        self.U = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(2 * p))  # First p elements\n",
    "        self.V = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(2 * p))  # Second p elements\n",
    "        self.W = nn.Parameter(self.init_scale * torch.randn(hidden_size, p) / np.sqrt(p)) # Second layer weights\n",
    "        print(f\"Initialized U with shape {self.U.shape}\")\n",
    "        print(f\"Initialized V with shape {self.V.shape}\")\n",
    "        print(f\"Initialized W with shape {self.W.shape}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input x shape: {x.shape}\")\n",
    "        # First layer (linear and combined)\n",
    "        x1 = x[:, :self.p] @ self.U.T\n",
    "        print(f\"x1 (x @ U.T) shape: {x1.shape}\")\n",
    "        x2 = x[:, self.p:] @ self.V.T\n",
    "        print(f\"x2 (x @ V.T) shape: {x2.shape}\")\n",
    "        x_combined = x1 + x2\n",
    "        print(f\"x_combined (x1 + x2) shape: {x_combined.shape}\")\n",
    "\n",
    "        # Apply nonlinearity activation\n",
    "        if self.nonlinearity == 'relu':\n",
    "            x_combined = torch.relu(x_combined)\n",
    "            print(\"Applied ReLU nonlinearity\")\n",
    "        elif self.nonlinearity == 'square':\n",
    "            x_combined = x_combined**2\n",
    "            print(\"Applied square nonlinearity\")\n",
    "        elif self.nonlinearity == 'linear':\n",
    "            x_combined = x_combined\n",
    "            print(\"Applied linear (identity) nonlinearity\")\n",
    "        elif self.nonlinearity == 'tanh':\n",
    "            x_combined = torch.tanh(x_combined)\n",
    "            print(\"Applied tanh nonlinearity\")\n",
    "        elif self.nonlinearity == 'gelu':\n",
    "            gelu = torch.nn.GELU()\n",
    "            x_combined = gelu(x_combined)\n",
    "            print(\"Applied GELU nonlinearity\")\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid nonlinearity '{self.nonlinearity}' provided.\")\n",
    "\n",
    "        # Second layer (linear)\n",
    "        x_out = x_combined @ self.W\n",
    "        print(f\"x_out (x_combined @ W) shape: {x_out.shape}\")\n",
    "\n",
    "        # Feature learning scaling\n",
    "        x_out *= self.output_scale\n",
    "        print(f\"x_out after scaling with output_scale={self.output_scale}: shape {x_out.shape}\")\n",
    "        \n",
    "        return x_out"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "2cf22b7d-49e7-445b-8742-2e75cd1fa55a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ],
   "id": "f7e7336b-5c6e-48af-a357-2b2c877f6168"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(\"Starting test_accuracy evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten input for FC layers\n",
    "            print(f\"Batch {i+1}: inputs reshaped\")\n",
    "            outputs = model(inputs)\n",
    "            print(f\"Batch {i+1}: model forward pass done\")\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the largest value (class)\n",
    "            _, true_labels = torch.max(labels, 1)  # Get the true class from the one-hot encoding\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            print(f\"Batch {i+1}: accuracy updated (correct={correct}, total={total})\")\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Final test accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, epochs=100, verbose_interval=10):\n",
    "    print(\"Starting training loop...\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    print(\"Model set to train mode.\")\n",
    "    loss_history = []  # List to store loss values\n",
    "    accuracy_history = []\n",
    "    param_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} started.\")\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.shape[0], -1)  # Flatten input for FC layers\n",
    "            print(f\"  Batch {batch_idx+1}: inputs reshaped\")\n",
    "\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            print(f\"  Batch {batch_idx+1}: optimizer gradients zeroed\")\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            print(f\"  Batch {batch_idx+1}: model forward pass done\")\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            print(f\"  Batch {batch_idx+1}: loss computed ({loss.item():.4f})\")\n",
    "            loss.backward()  # Backpropagation\n",
    "            print(f\"  Batch {batch_idx+1}: backward pass done\")\n",
    "            optimizer.step()  # Update weights\n",
    "            print(f\"  Batch {batch_idx+1}: optimizer step done\")\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            print(f\"  Batch {batch_idx+1}: running_loss updated ({running_loss:.4f})\")\n",
    "\n",
    "        # Append the average loss for the epoch to loss_history\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}: avg_loss appended ({avg_loss:.4f})\")\n",
    "\n",
    "        # Append the accuracy\n",
    "        model.eval()\n",
    "        print(f\"Epoch {epoch+1}: model set to eval mode for accuracy check\")\n",
    "        accuracy = test_accuracy(model, dataloader)\n",
    "        accuracy_history.append(accuracy)\n",
    "        print(f\"Epoch {epoch+1}: accuracy appended ({accuracy:.2f}%)\")\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch+1}: model set back to train mode\")\n",
    "\n",
    "        # Save current model parameters\n",
    "        current_params = {\n",
    "            \"U\": model.U.detach().cpu().clone(),\n",
    "            \"V\": model.V.detach().cpu().clone(),\n",
    "            \"W\": model.W.detach().cpu().clone()\n",
    "        }\n",
    "        param_history.append(current_params)\n",
    "        print(f\"Epoch {epoch+1}: model parameters saved\")\n",
    "\n",
    "        # Print verbose information every `verbose_interval` epochs\n",
    "        if (epoch + 1) % verbose_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training loop finished.\")\n",
    "    return loss_history, accuracy_history, param_history # Return loss history for plotting"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "1035f81c-e877-4655-8640-4e4c3d323af8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ],
   "id": "0e86c4f6-83a6-4465-abf0-7d104432cc9c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def style_axes(ax, numyticks=5, numxticks=5, labelsize=24):\n",
    "    # Y-axis ticks\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", bottom=True, top=False,\n",
    "                   labelbottom=True, left=True, right=False,\n",
    "                   labelleft=True, direction='out', length=7, width=1.5, pad=8, labelsize=labelsize)\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=numyticks))\n",
    "    \n",
    "    # X-axis ticks\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=True, top=False,\n",
    "                   labelbottom=True, left=True, right=False,\n",
    "                   labelleft=True, direction='out', length=7, width=1.5, pad=8, labelsize=labelsize)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=numxticks))\n",
    "\n",
    "    # Scientific notation formatting\n",
    "    if ax.get_yscale() == 'linear':\n",
    "        ax.ticklabel_format(style='sci', axis='y', scilimits=(-2, 2))\n",
    "    if ax.get_xscale() == 'linear':\n",
    "        ax.ticklabel_format(style='sci', axis='x', scilimits=(-2, 2))\n",
    "\n",
    "    ax.xaxis.offsetText.set_fontsize(20)\n",
    "    ax.grid()\n",
    "\n",
    "    # Customize spines\n",
    "    for spine in [\"top\", \"right\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    for spine in [\"left\", \"bottom\"]:\n",
    "        ax.spines[spine].set_linewidth(3)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "014e2d10-9550-4fd4-adb7-168a27fda1b3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_power(points):\n",
    "    p = len(points)\n",
    "    num_coefficients = (p // 2) + 1\n",
    "    \n",
    "    # Perform FFT and calculate power spectrum\n",
    "    ft = np.fft.fft(points) # Could consider using np.fft.rfft which is designed for real valued input.\n",
    "    power = np.abs(ft[:num_coefficients])**2 / p\n",
    "    \n",
    "    # Double power for frequencies strictly between 0 and Nyquist (Nyquist is not doubled if p is even)\n",
    "    if p % 2 == 0:  # p is even, Nyquist frequency at index num_coefficients - 1\n",
    "        power[1:num_coefficients - 1] *= 2\n",
    "    else:  # p is odd, no Nyquist frequency\n",
    "        power[1:] *= 2\n",
    "\n",
    "    # Confirm the power sum approximates the squared norm of points\n",
    "    total_power = np.sum(power)\n",
    "    norm_squared = np.linalg.norm(points)**2\n",
    "    if not np.isclose(total_power, norm_squared, rtol=1e-3):\n",
    "        print(f\"Warning: Total power {total_power:.3f} does not match norm squared {norm_squared:.3f}\")\n",
    "\n",
    "    return np.arange(num_coefficients), power\n",
    "\n",
    "def interpolate(ax, points, color, continuous, alpha=1.0):\n",
    "    p = len(points)\n",
    "    if continuous:\n",
    "        # Perform Fourier Transform\n",
    "        ft = np.fft.fft(points)\n",
    "        \n",
    "        # Keep only non-negative frequencies (first half + Nyquist if p is even)\n",
    "        num_coefficients = (p // 2) + 1\n",
    "        ft = ft[:num_coefficients]  # Truncate to keep non-negative frequencies\n",
    "        \n",
    "        # Create a dense set of x-values for smooth interpolation\n",
    "        xs = np.linspace(0, p, 10 * p)  # 10 times more points than the original for smoothness\n",
    "        curr_val = np.zeros(xs.shape, dtype=complex)\n",
    "        \n",
    "        # Use only non-negative frequencies for interpolation\n",
    "        for freq in range(num_coefficients):\n",
    "            theta = np.angle(ft[freq])\n",
    "            r = np.abs(ft[freq]) / p\n",
    "            # Double amplitude except for DC (freq = 0) and Nyquist (freq = p / 2, when p is even)\n",
    "            if freq > 0 and (freq < p / 2 or p % 2 != 0):\n",
    "                r *= 2\n",
    "            curr_val += r * np.exp(1j * ((2 * np.pi * freq * xs / p) + theta))\n",
    "\n",
    "        # Plot the real part (since output is real-valued)\n",
    "        ax.plot(xs, curr_val.real, color=color, alpha=alpha)\n",
    "    else:\n",
    "        ax.plot(np.arange(p), points, color=color, alpha=alpha)   "
   ],
   "execution_count": null,
   "outputs": [],
   "id": "20989d96-f34f-4be7-a0f9-4b92fb7f235a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Experiment"
   ],
   "id": "e99dae27-f8fe-403a-b70f-0bcaf818cbe7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "seed = 0  # or any integer you like\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "\n",
    "# Data Generation using the new function\n",
    "# TEST_MODE: Reduce p and hidden_size for faster automated testing\n",
    "import os\n",
    "TEST_MODE = os.environ.get(\"NOTEBOOK_TEST_MODE\", \"0\") == \"1\"\n",
    "p = 20  # Keep same value in TEST_MODE to avoid index errors  # Modulus (reduced in test mode)\n",
    "\n",
    "# Get base vector\n",
    "# template = generate_template(p, 2, 1.0)\n",
    "# template = one_hot(p)\n",
    "template = generate_fixed_template(p)\n",
    "\n",
    "# Mean center template\n",
    "template -= np.mean(template)\n",
    "\n",
    "# Generate dataset using numpy\n",
    "X, Y = ModularAdditionDataset(p, template)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).view(-1, 2 * p)  # Flatten input (num_samples, 2*p)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)  # Targets (num_samples, p)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "hidden_size = 6 if TEST_MODE else 6 * 3  # Reduced in test mode\n",
    "model = TwoLayerNet(p=p, hidden_size=hidden_size, nonlinearity='square', init_scale=1e-2, output_scale=1e0)\n",
    "\n",
    "# Create loss function\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Construct optimizer\n",
    "lr, mom = 0.01, 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=mom)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Train the model\n",
    "# TEST_MODE: Set to reduce epochs for automated testing\n",
    "import os\n",
    "TEST_MODE = os.environ.get(\"NOTEBOOK_TEST_MODE\", \"0\") == \"1\"\n",
    "epochs = 2 if TEST_MODE else 1000001\n",
    "loss_history, accuracy_history, param_history = train(model, dataloader, loss, optimizer, epochs=epochs, verbose_interval=max(1, epochs//100))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "bcd15c5a-5745-41ba-b015-48e403160c7e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGF Numerics"
   ],
   "id": "eae371c4-1405-4ac5-982c-0ebacb688ed7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ModsumSubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, init_scale):\n",
    "        super().__init__()\n",
    "        assert d_in%2 == 0\n",
    "        self.p = d_in // 2\n",
    "        self.u = nn.Linear(self.p, 1, bias=False)\n",
    "        self.v = nn.Linear(self.p, 1, bias=False)\n",
    "        self.w = nn.Linear(1, d_out, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.w.weight.mul_(init_scale)\n",
    "            self.u.weight.mul_(init_scale)\n",
    "            self.v.weight.mul_(init_scale)\n",
    "        self.active = False\n",
    "        self.util_acc = 0\n",
    "        self.c_a = 1/self.get_norm() - 1\n",
    "        \n",
    "        self.normalize()\n",
    "        \n",
    "    def get_norm(self):\n",
    "        sqnorm = lambda x: torch.linalg.norm(x.weight)**2\n",
    "        norm = torch.sqrt(sqnorm(self.w) + sqnorm(self.u) + sqnorm(self.v))\n",
    "        return norm\n",
    "    \n",
    "    def reinitialize(self, u, v, w):\n",
    "        with torch.no_grad():\n",
    "            self.u.weight.copy_(u)\n",
    "            self.v.weight.copy_(v)\n",
    "            self.w.weight.copy_(w)\n",
    "        self.c_a = 1/self.get_norm() - 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :self.p]\n",
    "        x2 = x[:, self.p:]\n",
    "        return self.w((self.u(x1) + self.v(x2))**2)\n",
    "    \n",
    "    def normalize(self):\n",
    "        norm = self.get_norm()\n",
    "        with torch.no_grad():\n",
    "            self.w.weight.div_(norm)\n",
    "            self.u.weight.div_(norm)\n",
    "            self.v.weight.div_(norm)\n",
    "    \n",
    "    def utility_step(self, x, residual, learning_rate):\n",
    "        f_i = self(x)\n",
    "        util = torch.einsum('nd,nd->n',  f_i, residual).mean()\n",
    "        self.util_acc += 3 * learning_rate * util.item()\n",
    "        norm_th = 1/(1 + self.c_a - self.util_acc)\n",
    "        \n",
    "        util.backward()\n",
    "        with torch.no_grad():\n",
    "            self.w.weight += norm_th * learning_rate * self.w.weight.grad\n",
    "            self.u.weight += norm_th * learning_rate * self.u.weight.grad\n",
    "            self.v.weight += norm_th * learning_rate * self.v.weight.grad\n",
    "            self.w.weight.grad.zero_()\n",
    "            self.u.weight.grad.zero_()\n",
    "            self.v.weight.grad.zero_()\n",
    "            self.normalize()\n",
    "\n",
    "\n",
    "class ModsumNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, init_scale, width=100):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.width = width\n",
    "        neurons = [ModsumSubNetwork(d_in, d_out, init_scale) for _ in range(width)]\n",
    "        self.neurons = nn.ModuleList(neurons)\n",
    "        self.set_mode(\"utilmax\")\n",
    "    \n",
    "    def load_init(self, U, V, W):\n",
    "        for i, n in enumerate(self.neurons):\n",
    "            u, v, w = U[i], V[i], W[i][:, None]\n",
    "            n.reinitialize(u, v, w)\n",
    "\n",
    "    def dormant(self):\n",
    "        return [neuron for neuron in self.neurons if not neuron.active]\n",
    "    \n",
    "    def active(self):\n",
    "        return [neuron for neuron in self.neurons if neuron.active]\n",
    "\n",
    "            \n",
    "    def set_mode(self, mode):\n",
    "        if mode not in [\"utilmax\", \"costmin\"]:\n",
    "            raise ValueError(\"mode must be utilmax or costmin\")\n",
    "        self.mode = mode\n",
    "        for neuron in self.neurons:\n",
    "            grad_on = (mode==\"utilmax\") ^ neuron.active\n",
    "            for param in neuron.parameters():\n",
    "                param.requires_grad = grad_on\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not np.any([n.active for n in self.neurons]):\n",
    "            return torch.zeros(x.shape[0], self.d_out)\n",
    "        else:\n",
    "            outputs = torch.stack([neuron(x) for neuron in self.neurons if neuron.active], dim=0)\n",
    "            return torch.sum(outputs, dim=0)\n",
    "\n",
    "\n",
    "def train_agf(X_train, Y_train, init_sz=1e-3, agf_steps=5, from_init=None, \n",
    "              utilmax_lr=1, costmin_lr=1, costmin_maxiter=1e4, loss_thresh=1e-4):\n",
    "    \n",
    "    # Initialize\n",
    "    d_in, d_out = X_train.shape[-1], Y_train.shape[-1]\n",
    "    if from_init:\n",
    "        U, V, W = from_init[\"U\"], from_init[\"V\"], from_init[\"W\"]\n",
    "        assert d_in == U.shape[1]*2\n",
    "        assert d_out == W.shape[1]\n",
    "        width = U.shape[0]\n",
    "        net = ModsumNetwork(d_in, d_out, init_sz, width=width)#.cuda()\n",
    "        net.load_init(U, V, W)\n",
    "    else:\n",
    "        net = ModsumNetwork(d_in, d_out, init_sz, width=agf_steps)#.cuda()\n",
    "    X_train.requires_grad = False\n",
    "    \n",
    "    def update_results(results, t):\n",
    "        results[\"t\"].append(t)\n",
    "        residual = (Y_train - net(X_train))\n",
    "        residual = residual.detach()\n",
    "        results[\"residuals\"].append(residual)\n",
    "        loss = (residual**2).mean().item()\n",
    "        results[\"losses\"].append(loss)\n",
    "        results[\"models\"].append(net.state_dict())\n",
    "        results[\"pred\"].append(net(X_train).detach().cpu().clone())\n",
    "        \n",
    "    results = {\n",
    "        \"t\": [],\n",
    "        \"residuals\": [],\n",
    "        \"losses\": [],\n",
    "        \"models\": [],\n",
    "        \"pred\": [],\n",
    "    }\n",
    "\n",
    "    t = 0\n",
    "    update_results(results, t)\n",
    "    for _ in tqdm(range(agf_steps)):\n",
    "        \n",
    "        # Utility Maximization\n",
    "        residual = (1/d_out) * 2*(Y_train - net(X_train))\n",
    "        residual = residual.detach()\n",
    "        iters = 0\n",
    "        mode = \"utilmax\"\n",
    "        while mode == \"utilmax\":\n",
    "            for n in net.neurons:\n",
    "                if n.active:\n",
    "                    continue\n",
    "                n.utility_step(X_train, residual, utilmax_lr)\n",
    "                if n.util_acc > n.c_a:\n",
    "                    n.active = True\n",
    "                    mode = \"costmin\"\n",
    "                    # break\n",
    "            iters += 1\n",
    "        net.set_mode(mode)\n",
    "        t += iters\n",
    "\n",
    "        # Cost Minimization\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=costmin_lr, momentum=0.9)\n",
    "        for i in range(int(costmin_maxiter)):\n",
    "            optimizer.zero_grad(set_to_none=False)\n",
    "            residual = Y_train - net(X_train)\n",
    "            loss = (residual ** 2).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        net.set_mode(\"utilmax\")\n",
    "\n",
    "        \n",
    "        print(f\"loss: {loss.item():.5f}\")\n",
    "        update_results(results, t)\n",
    "\n",
    "        # Check for Termination\n",
    "        if not net.dormant() or loss.item() < loss_thresh:\n",
    "            break\n",
    "    \n",
    "    return results"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "489e82e1-61c8-43e6-b260-fd96c815dec8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "costmin_lr = 0.01\n",
    "utilmax_lr = 0.1\n",
    "results = train_agf(X_tensor, Y_tensor, init_sz=model.init_scale, agf_steps=50, from_init=param_history[0],\n",
    "                    utilmax_lr=utilmax_lr, costmin_lr=costmin_lr,\n",
    "                    costmin_maxiter=1e4, loss_thresh=1e-4)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "5b3eddcc-8c3e-45dd-8f57-45585a021f5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ],
   "id": "0f48aebc-a439-405a-a057-3f5c24cca91a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(list(loss_history), lw=4)\n",
    "\n",
    "for lossval in results[\"losses\"]:\n",
    "    ax.axhline(lossval, alpha=0.3, ls=\":\", color=\"xkcd:slate\", zorder=-4, lw=2)\n",
    "\n",
    "f = utilmax_lr / (lr/(1-mom))\n",
    "for t in results[\"t\"]:\n",
    "    ax.axvline(f*t, alpha=0.3, ls=\":\", color=\"xkcd:slate\", zorder=-4, lw=2)\n",
    "\n",
    "times = results[\"t\"] + [epochs]\n",
    "AGF_losses = results[\"losses\"] + [results[\"losses\"][-1]]\n",
    "ax.step(f*np.array(times), AGF_losses, where=\"post\", lw=2, ls='dashed', color=\"k\")\n",
    "\n",
    "# === Compute power spectrum of template ===\n",
    "freq, power = get_power(template)\n",
    "valid = power > 1e-20\n",
    "freq, power = freq[valid], power[valid]\n",
    "sorted_idx = np.argsort(-power)\n",
    "freq, power = freq[sorted_idx], power[sorted_idx]\n",
    "\n",
    "alpha_values = [np.sum(power[k:]) for k in range(len(power))]\n",
    "coef = 1 / p\n",
    "for k, alpha in enumerate(alpha_values):\n",
    "    ax.axhline(y=coef * alpha, color='black', linestyle='--', linewidth=2, zorder=-2)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlim(1e1, 1e6)\n",
    "ax.set_ylim(1e-3, 1e0)\n",
    "ax.set_xlabel('Epochs', fontsize=24)\n",
    "ax.set_ylabel('Train Loss', fontsize=24)\n",
    "\n",
    "style_axes(ax)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss-without-lines.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "ff46febe-abb5-459a-bb06-a18a26afb967"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Spectrum of output"
   ],
   "id": "40b851e7-6256-43cd-b9f3-aca38db04917"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === SETTINGS ===\n",
    "p = Y_tensor.shape[1]\n",
    "num_freqs = p // 2 + 1\n",
    "\n",
    "# Compute template power spectrum\n",
    "template_ft = np.fft.rfft(template)\n",
    "template_power = np.abs(template_ft)[:num_freqs]\n",
    "\n",
    "# === Compute power spectrum of template ===\n",
    "freq, power = get_power(template)\n",
    "valid = power > 1e-20\n",
    "freq, power = freq[valid], power[valid]\n",
    "sorted_idx = np.argsort(-power)\n",
    "freq, power = freq[sorted_idx], power[sorted_idx]\n",
    "\n",
    "# === Theory lines ===\n",
    "alpha_values = [np.sum(power[k:]) for k in range(len(power))]\n",
    "coef = 1 / p\n",
    "theta0 = np.sqrt(2) * model.init_scale\n",
    "uMax = [np.sqrt(2 * p / 27) * (p * power[k] / 2)**(3/2) / p**2 for k in range(len(power))]\n",
    "tau_values = [(1 / theta0 - 1) / (3 * uMax[k]) for k in range(len(uMax))]\n",
    "step_size = 2 * coef * lr / (1 - mom)\n",
    "\n",
    "\n",
    "# Color settings\n",
    "cmap = plt.colormaps.get_cmap('tab20').resampled(num_freqs)\n",
    "manual_colors = {\n",
    "    0: 'tab:blue',\n",
    "    1: 'tab:orange',\n",
    "    2: 'tab:red',\n",
    "    3: 'tab:green',\n",
    "    4: 'tab:brown',\n",
    "    5: 'tab:purple',\n",
    "}\n",
    "colors = [manual_colors.get(i, cmap(i)) for i in range(num_freqs)]\n",
    "\n",
    "# Compute output power over time (GD)\n",
    "num_points = 1000\n",
    "steps = np.unique(np.logspace(0, np.log10(len(param_history) - 1), num_points, dtype=int))\n",
    "powers_over_time = []\n",
    "\n",
    "for step in steps:\n",
    "    model.load_state_dict(param_history[step])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        ft = np.fft.rfft(outputs.detach().cpu().numpy(), axis=1)\n",
    "        avg_power = np.mean(np.abs(ft), axis=0)\n",
    "        powers_over_time.append(avg_power)\n",
    "\n",
    "powers_over_time = np.array(powers_over_time)  # shape: (steps, freqs)\n",
    "\n",
    "\n",
    "# Compute output power over time (AGF)\n",
    "f = utilmax_lr / (lr/(1-mom))\n",
    "AGF_steps = results[\"t\"]\n",
    "powers_over_time_AGF = []\n",
    "for i, step in enumerate(AGF_steps):\n",
    "    outputs = results[\"pred\"][i]\n",
    "    ft = np.fft.rfft(outputs.detach().cpu().numpy(), axis=1)\n",
    "    avg_power = np.mean(np.abs(ft), axis=0)\n",
    "    powers_over_time_AGF.append(avg_power)\n",
    "powers_over_time_AGF = np.array(powers_over_time_AGF)  # shape: (steps, freqs)\n",
    "AGF_steps = [f * t for t in AGF_steps]\n",
    "\n",
    "AGF_steps.append(epochs)\n",
    "powers_over_time_AGF = np.vstack([\n",
    "    powers_over_time_AGF,\n",
    "    powers_over_time_AGF[-1, :]\n",
    "])\n",
    "\n",
    "# === PLOTTING ===\n",
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "\n",
    "for k in range(num_freqs):\n",
    "    color = colors[k]\n",
    "    label = fr\"$\\xi = {k}$\" if k in [1, 3, 5] else None\n",
    "    ax.plot(steps, powers_over_time[:, k], color=color, lw=3, label=label)\n",
    "    label_agf = 'AGF' if k == 10 else None\n",
    "    ax.step(AGF_steps, powers_over_time_AGF[:, k], color='k', lw=2, ls='dashed', where=\"post\", label=label_agf)\n",
    "    ax.axhline(template_power[k], color=color, linestyle='dotted', linewidth=2, alpha=0.5, zorder=-10)\n",
    "\n",
    "for k, tau in enumerate(tau_values):\n",
    "    color = colors[freq[k]]\n",
    "    ax.axvline(x=tau / step_size, color=color, linestyle='dashed', linewidth=2, alpha=0.5)\n",
    "\n",
    "    # Add arrow at intersection\n",
    "    x = tau / step_size\n",
    "    y = template_power[freq[k]]\n",
    "    #draw an arrow from the lower bound to the right\n",
    "    #use default color cycle\n",
    "    ax.arrow(1.04 * x, y + 0.5, 1.5 * x, 0, \n",
    "             head_width=0.2, head_length=x*0.2, length_includes_head=True,\n",
    "             fc=color, ec=color, lw=4)\n",
    "\n",
    "# # Add vertical lines if needed\n",
    "# for step in time_steps:\n",
    "#     ax.axvline(x=step, color='gray', alpha=0.5, linestyle='solid', linewidth=2)\n",
    "\n",
    "# Labeling and formatting\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(5e1, 2e6)\n",
    "ax.set_xticks([1000, 10000, 100000, epochs-1])\n",
    "ax.set_ylabel(\"Power\", fontsize=24)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=24)\n",
    "ax.legend(fontsize=14, title=\"Frequency\", title_fontsize=16, loc='upper right', bbox_to_anchor=(1, 0.9), labelspacing=0.25)\n",
    "\n",
    "style_axes(ax)\n",
    "ax.set_xticks([1000, 10000, 100000, epochs-1])\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fourier_power_only.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "68b25ca9-6339-49dd-9d45-577a51798a25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot outputs"
   ],
   "id": "5ef2c971-d9f1-41e6-b8eb-4e467496ccfd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose time steps to visualize\n",
    "steps_to_show = [1000, 10000, 100000, epochs-1]\n",
    "num_samples = 1  # how many examples to plot per row\n",
    "p = Y_tensor.shape[1]\n",
    "x = np.arange(p)\n",
    "\n",
    "fig, axes = plt.subplots(len(steps_to_show), 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "for row, step in enumerate(steps_to_show):\n",
    "    # Load weights at this step\n",
    "    model.load_state_dict(param_history[step])\n",
    "    model.eval()\n",
    "\n",
    "    indices = np.random.choice(len(Y_tensor), size=num_samples, replace=False)\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_tensor[indices]).detach().cpu().numpy()\n",
    "        truths = Y_tensor[indices].detach().cpu().numpy()\n",
    "\n",
    "    ax = axes[row]\n",
    "    for i, idx in enumerate(indices):\n",
    "        a = idx // p\n",
    "        b = idx % p\n",
    "        label_true = r\"$(a + b) \\cdot x$\"\n",
    "        label_pred = r\"$f(a \\cdot x, b \\cdot x)$\"\n",
    "\n",
    "        # Plot ground truth\n",
    "        interpolate(ax, truths[i], color=f\"C{i}\", alpha=0.9, continuous=True)\n",
    "        ax.scatter(x, truths[i], color=f\"C{i}\", s=30, alpha=0.9, label=label_true)\n",
    "\n",
    "        # Plot prediction\n",
    "        interpolate(ax, preds[i], color='k', alpha=1.0, continuous=True)\n",
    "        ax.scatter(x, preds[i], color='k', s=30, alpha=0.7, label=label_pred)\n",
    "\n",
    "    style_axes(ax, numyticks=3, labelsize=12)\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel(fr\"$t = 10^{{{int(np.log10(step))}}}$\", fontsize=20)\n",
    "\n",
    "    # Only bottom row gets x-ticks\n",
    "    if row < len(steps_to_show) - 1:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "\n",
    "    # ax.legend(loc='best', fontsize=12, title=fr\"$a = {a}, b = {b}$\", handlelength=0, labelspacing=0.1, title_fontsize=14, frameon=False)\n",
    "    ax.legend(\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(0.95, 0.5),  # X slightly beyond the right edge, Y centered\n",
    "        fontsize=8,\n",
    "        title=fr\"$a = {a}, b = {b}$\",\n",
    "        title_fontsize=10,\n",
    "        handlelength=0,\n",
    "        labelspacing=0.1,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "# axes[-1].set_xlabel(\"Output Index\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"predictions.pdf\", bbox_inches='tight')"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e333d1ab-1501-434f-86d2-82c10bb58f11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Weights"
   ],
   "id": "b267424b-a0e5-47e3-9e01-1dc41e05e026"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Steps and corresponding highlighted frequencies\n",
    "\n",
    "steps = [1000, 10000, 100000, epochs-1]\n",
    "highlight_freqs_list = [[], [1], [3], [5]]\n",
    "\n",
    "num_rows, num_cols = len(steps), 3\n",
    "\n",
    "# Use gridspec to control layout\n",
    "fig = plt.figure(figsize=(24, 6), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(num_rows, num_cols, width_ratios=[1.1, 1.1, 2.0], wspace=0.1, hspace=0.1)\n",
    "axes = np.empty((num_rows, num_cols), dtype=object)\n",
    "\n",
    "# Create axes\n",
    "for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "        if col == 2:\n",
    "            ax = fig.add_subplot(gs[row, col], projection='polar')\n",
    "        else:\n",
    "            ax = fig.add_subplot(gs[row, col])  # \u2b05 no sharex anymore\n",
    "        axes[row, col] = ax\n",
    "\n",
    "num_freqs = None\n",
    "for row, index in enumerate(steps):\n",
    "    highlight_freqs = highlight_freqs_list[row]\n",
    "    params = param_history[index]\n",
    "    W = params['W'].numpy()\n",
    "    h, p = W.shape\n",
    "\n",
    "    if num_freqs is None:\n",
    "        num_freqs = p // 2 + 1\n",
    "        cmap = plt.colormaps.get_cmap('tab20').resampled(num_freqs)\n",
    "        colors = [cmap(i) for i in range(num_freqs)]\n",
    "        manual_colors = {\n",
    "            0: 'tab:blue',\n",
    "            1: 'tab:orange',\n",
    "            2: 'tab:red',\n",
    "            3: 'tab:green',\n",
    "            4: 'tab:brown',\n",
    "            5: 'tab:purple',\n",
    "        }\n",
    "        freq_colors = [manual_colors.get(i, cmap(i)) for i in range(num_freqs)]\n",
    "\n",
    "\n",
    "    # === Column 1: Weights ===\n",
    "    ax = axes[row, 0]\n",
    "    for i in range(h):\n",
    "        w = W[i, :]\n",
    "        ft = np.fft.rfft(w)\n",
    "        power = np.abs(ft)**2\n",
    "        dom_idx = np.argmax(power)\n",
    "        color = freq_colors[dom_idx]\n",
    "        alpha = 0.9 if not highlight_freqs or dom_idx in highlight_freqs else 0.1\n",
    "        x = np.linspace(0, p - 1, 500)\n",
    "        interpolate(ax, w, color=color, continuous=True, alpha=alpha)\n",
    "        ax.scatter(np.arange(p), w, color=color, s=10, alpha=alpha)\n",
    "    if row == 0: ax.set_title(\"Weights\", fontsize=24)\n",
    "    ax.set_ylabel(fr\"$t = 10^{{{int(np.log10(index))}}}$\", fontsize=20)\n",
    "    style_axes(ax, numyticks=3, numxticks=5, labelsize=12)\n",
    "    ax.grid(False)\n",
    "    if row < num_rows - 1:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "\n",
    "    # === Column 2: Frequency Spectrum ===\n",
    "    ax = axes[row, 1]\n",
    "    for i in range(h):\n",
    "        w = W[i, :]\n",
    "        ft = np.fft.rfft(w)\n",
    "        power = np.abs(ft)**2\n",
    "        for k in range(len(power)):\n",
    "            color = freq_colors[k]\n",
    "            ax.vlines(k, 0, power[k], linewidth=4, color=color, alpha=0.4)\n",
    "            ax.scatter(k, power[k], color=color, s=50, alpha=0.7)\n",
    "    # ax.axhline(0, color='gray', linewidth=1, linestyle='--', alpha=0.4)\n",
    "    ax.set_xlim(-0.5, len(power) - 0.5)\n",
    "    ax.set_xticks(np.arange(len(power)))\n",
    "    if row == 0: ax.set_title(\"Frequency\", fontsize=24)\n",
    "    style_axes(ax, numyticks=3, numxticks=11, labelsize=12)\n",
    "    ax.grid(False)\n",
    "    if row < num_rows - 1:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "\n",
    "    # === Column 3: Phase Polar Plot ===\n",
    "    ax = axes[row, 2]\n",
    "    for i in range(h):\n",
    "        w = W[i, :]\n",
    "        ft = np.fft.rfft(w)\n",
    "        power = np.abs(ft)**2\n",
    "        dom_idx = np.argmax(power)\n",
    "        phase = np.angle(ft[dom_idx])\n",
    "        norm = np.linalg.norm(w)\n",
    "        color = freq_colors[dom_idx]\n",
    "        alpha = 0.9 if not highlight_freqs or dom_idx in highlight_freqs else 0.1\n",
    "        ax.plot([phase, phase], [0, norm], color=color, linewidth=2, alpha=alpha)\n",
    "        ax.scatter(phase, norm, color=color, s=40, alpha=alpha)\n",
    "        angles = np.arange(0, 360, 45)\n",
    "        # ax.set_thetagrids(angles, [f\"{a}\u00b0\" if a in [45,135,225,315] else \"\" for a in angles])\n",
    "        ax.set_thetagrids(angles, [\"\" for a in angles])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.spines['polar'].set_linewidth(2)\n",
    "    if row == 0: ax.set_title(\"Phase\", fontsize=24)\n",
    "\n",
    "# Shift polar plots left to reduce whitespace\n",
    "for row in range(num_rows):\n",
    "    ax = axes[row, 2]\n",
    "    pos = ax.get_position()\n",
    "    ax.set_position([pos.x0 - 0.155, pos.y0, pos.width, pos.height])\n",
    "\n",
    "plt.savefig(\"W-weights.pdf\", bbox_inches='tight')"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "9de707bf-838e-4384-8150-3d8fe4586fc3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": [],
   "id": "9f5f56f9-7055-4056-9a18-7d91b3be50f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}